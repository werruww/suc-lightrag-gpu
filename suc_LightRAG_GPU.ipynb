{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjhBC0LQVPUo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-pgdBIwNVXTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_pSg8LfZVXYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/HKUDS/LightRAG.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_s5XXNoVXdG",
        "outputId": "906e0d97-e2d0-4b13-81df-24ffedd7ec24"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LightRAG'...\n",
            "remote: Enumerating objects: 19837, done.\u001b[K\n",
            "remote: Counting objects: 100% (100/100), done.\u001b[K\n",
            "remote: Compressing objects: 100% (57/57), done.\u001b[K\n",
            "remote: Total 19837 (delta 58), reused 43 (delta 43), pack-reused 19737 (from 3)\u001b[K\n",
            "Receiving objects: 100% (19837/19837), 47.01 MiB | 11.86 MiB/s, done.\n",
            "Resolving deltas: 100% (14040/14040), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd LightRAG\n",
        "!pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JmyAIzorVZNP",
        "outputId": "09f3ec98-1114-427a-e641-1bce51b088cf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LightRAG\n",
            "Obtaining file:///content/LightRAG\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from lightrag-hku==1.3.8) (3.11.15)\n",
            "Collecting configparser (from lightrag-hku==1.3.8)\n",
            "  Downloading configparser-7.2.0-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from lightrag-hku==1.3.8) (1.0.0)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from lightrag-hku==1.3.8) (2.2.2)\n",
            "Collecting pipmaster (from lightrag-hku==1.3.8)\n",
            "  Downloading pipmaster-0.7.2-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from lightrag-hku==1.3.8) (2.11.4)\n",
            "Collecting python-dotenv (from lightrag-hku==1.3.8)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting pyuca (from lightrag-hku==1.3.8)\n",
            "  Downloading pyuca-1.2-py2.py3-none-any.whl.metadata (649 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightrag-hku==1.3.8) (75.2.0)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.11/dist-packages (from lightrag-hku==1.3.8) (9.1.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from lightrag-hku==1.3.8) (0.9.0)\n",
            "Collecting xlsxwriter>=3.1.0 (from lightrag-hku==1.3.8)\n",
            "  Downloading XlsxWriter-3.2.3-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->lightrag-hku==1.3.8) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->lightrag-hku==1.3.8) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->lightrag-hku==1.3.8) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->lightrag-hku==1.3.8) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->lightrag-hku==1.3.8) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->lightrag-hku==1.3.8) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->lightrag-hku==1.3.8) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->lightrag-hku==1.3.8) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->lightrag-hku==1.3.8) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->lightrag-hku==1.3.8) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->lightrag-hku==1.3.8) (1.20.0)\n",
            "Requirement already satisfied: packaging>=21.0 in /usr/local/lib/python3.11/dist-packages (from pipmaster->lightrag-hku==1.3.8) (24.2)\n",
            "Collecting ascii_colors>=0.8.0 (from pipmaster->lightrag-hku==1.3.8)\n",
            "  Downloading ascii_colors-0.11.4-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->lightrag-hku==1.3.8) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->lightrag-hku==1.3.8) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->lightrag-hku==1.3.8) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->lightrag-hku==1.3.8) (0.4.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->lightrag-hku==1.3.8) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->lightrag-hku==1.3.8) (2.32.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ascii_colors>=0.8.0->pipmaster->lightrag-hku==1.3.8) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->lightrag-hku==1.3.8) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->lightrag-hku==1.3.8) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->lightrag-hku==1.3.8) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->lightrag-hku==1.3.8) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->lightrag-hku==1.3.8) (2025.4.26)\n",
            "Downloading XlsxWriter-3.2.3-py3-none-any.whl (169 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading configparser-7.2.0-py3-none-any.whl (17 kB)\n",
            "Downloading pipmaster-0.7.2-py3-none-any.whl (25 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading pyuca-1.2-py2.py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ascii_colors-0.11.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyuca, xlsxwriter, python-dotenv, configparser, ascii_colors, pipmaster, lightrag-hku\n",
            "  Running setup.py develop for lightrag-hku\n",
            "Successfully installed ascii_colors-0.11.4 configparser-7.2.0 lightrag-hku-1.3.8 pipmaster-0.7.2 python-dotenv-1.1.0 pyuca-1.2 xlsxwriter-3.2.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "backports"
                ]
              },
              "id": "1beefd771a0e408f8acec1824adfe1e9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token XXXXXXXXXXXXX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nyTt-q2Wnct",
        "outputId": "e62cb930-4085-44b4-836f-4dea9ccde61f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "The token `read` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `read`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd LightRAG\n",
        "!python /content/LightRAG/examples/unofficial-sample/lightrag_hf_demo.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esc67r_BVffF",
        "outputId": "fb4684fe-4390-47f8-fc92-108bf4652f14"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'LightRAG'\n",
            "/content/LightRAG\n",
            "\u001b[94m2025-05-22 00:18:00 - pipmaster.package_manager - INFO - Targeting pip associated with Python: /usr/bin/python3 | Command base: /usr/bin/python3 -m pip\u001b[0m\n",
            "INFO: Process 1874 Shared-Data created for Single Process\n",
            "INFO: Process 1874 initialized updated flags for namespace: [full_docs]\n",
            "INFO: Process 1874 ready to initialize storage namespace: [full_docs]\n",
            "INFO: Process 1874 initialized updated flags for namespace: [text_chunks]\n",
            "INFO: Process 1874 ready to initialize storage namespace: [text_chunks]\n",
            "INFO: Process 1874 initialized updated flags for namespace: [entities]\n",
            "INFO: Process 1874 initialized updated flags for namespace: [relationships]\n",
            "INFO: Process 1874 initialized updated flags for namespace: [chunks]\n",
            "INFO: Process 1874 initialized updated flags for namespace: [chunk_entity_relation]\n",
            "INFO: Process 1874 initialized updated flags for namespace: [llm_response_cache]\n",
            "INFO: Process 1874 ready to initialize storage namespace: [llm_response_cache]\n",
            "INFO: Process 1874 initialized updated flags for namespace: [doc_status]\n",
            "INFO: Process 1874 ready to initialize storage namespace: [doc_status]\n",
            "INFO: Process 1874 storage namespace already initialized: [full_docs]\n",
            "INFO: Process 1874 storage namespace already initialized: [text_chunks]\n",
            "INFO: Process 1874 storage namespace already initialized: [llm_response_cache]\n",
            "INFO: Process 1874 storage namespace already initialized: [doc_status]\n",
            "INFO: Process 1874 Pipeline namespace initialized\n",
            "2025-05-22 00:18:07.595380: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747873087.616271    1874 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747873087.622184    1874 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-22 00:18:07.643879: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "tokenizer_config.json: 100% 54.5k/54.5k [00:00<00:00, 10.2MB/s]\n",
            "tokenizer.json: 100% 9.09M/9.09M [00:00<00:00, 26.0MB/s]\n",
            "special_tokens_map.json: 100% 296/296 [00:00<00:00, 2.54MB/s]\n",
            "config.json: 100% 878/878 [00:00<00:00, 6.73MB/s]\n",
            "model.safetensors.index.json: 100% 20.9k/20.9k [00:00<00:00, 74.7MB/s]\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "\n",
            "model-00002-of-00002.safetensors:   0% 0.00/1.46G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   0% 0.00/4.97G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 10.5M/1.46G [00:00<00:22, 63.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   0% 10.5M/4.97G [00:00<01:06, 74.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 31.5M/1.46G [00:00<00:12, 112MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   1% 31.5M/4.97G [00:00<00:38, 127MB/s] \u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 62.9M/1.46G [00:00<00:09, 154MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   1% 62.9M/4.97G [00:00<00:28, 169MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   6% 83.9M/1.46G [00:00<00:08, 160MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   2% 83.9M/4.97G [00:00<00:29, 166MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 105M/1.46G [00:00<00:08, 159MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   2% 105M/4.97G [00:00<00:28, 168MB/s] \u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 126M/1.46G [00:00<00:07, 169MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   3% 136M/4.97G [00:00<00:26, 184MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  10% 147M/1.46G [00:00<00:07, 180MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   3% 157M/4.97G [00:00<00:27, 175MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  12% 178M/1.46G [00:01<00:06, 189MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   4% 189M/4.97G [00:01<00:25, 189MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 210M/1.46G [00:01<00:06, 202MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   4% 210M/4.97G [00:01<00:25, 189MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 231M/1.46G [00:01<00:06, 203MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 252M/1.46G [00:01<00:06, 199MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   5% 231M/4.97G [00:01<00:28, 164MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  19% 273M/1.46G [00:01<00:06, 188MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   5% 252M/4.97G [00:01<00:35, 132MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 294M/1.46G [00:01<00:06, 167MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   5% 273M/4.97G [00:01<00:34, 136MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 315M/1.46G [00:01<00:07, 150MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   6% 294M/4.97G [00:01<00:36, 127MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 336M/1.46G [00:02<00:08, 139MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   6% 315M/4.97G [00:02<00:37, 124MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 357M/1.46G [00:02<00:07, 139MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   7% 336M/4.97G [00:02<00:36, 127MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  26% 377M/1.46G [00:02<00:09, 115MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   7% 357M/4.97G [00:02<00:37, 123MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 398M/1.46G [00:02<00:08, 132MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   8% 377M/4.97G [00:02<00:36, 126MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 419M/1.46G [00:02<00:07, 140MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   8% 398M/4.97G [00:02<00:34, 133MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 440M/1.46G [00:02<00:08, 120MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   8% 419M/4.97G [00:02<00:33, 136MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  32% 461M/1.46G [00:03<00:07, 128MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   9% 440M/4.97G [00:03<00:34, 132MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 482M/1.46G [00:03<00:07, 124MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   9% 461M/4.97G [00:03<00:34, 130MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  10% 482M/4.97G [00:03<00:33, 133MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 503M/1.46G [00:03<00:08, 112MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  10% 503M/4.97G [00:03<00:33, 134MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  36% 524M/1.46G [00:03<00:07, 123MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  11% 524M/4.97G [00:03<00:35, 124MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 545M/1.46G [00:03<00:07, 119MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  11% 545M/4.97G [00:03<00:32, 135MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  39% 566M/1.46G [00:03<00:07, 127MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 587M/1.46G [00:04<00:06, 136MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  11% 566M/4.97G [00:06<02:45, 26.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 608M/1.46G [00:06<00:30, 27.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  12% 598M/4.97G [00:06<01:46, 40.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 640M/1.46G [00:06<00:19, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 661M/1.46G [00:06<00:14, 54.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  13% 629M/4.97G [00:06<01:15, 57.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  13% 650M/4.97G [00:06<01:02, 69.1MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  47% 682M/1.46G [00:06<00:11, 66.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 703M/1.46G [00:06<00:09, 82.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  14% 671M/4.97G [00:06<00:52, 82.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  50% 724M/1.46G [00:06<00:07, 96.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  14% 703M/4.97G [00:06<00:40, 105MB/s] \u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 744M/1.46G [00:06<00:06, 111MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  15% 734M/4.97G [00:06<00:33, 126MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 776M/1.46G [00:07<00:05, 136MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  15% 765M/4.97G [00:07<00:29, 143MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 807M/1.46G [00:07<00:04, 155MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  16% 786M/4.97G [00:07<00:29, 142MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 828M/1.46G [00:07<00:04, 141MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  16% 807M/4.97G [00:07<00:32, 128MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 849M/1.46G [00:07<00:04, 141MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  17% 828M/4.97G [00:07<00:33, 124MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 870M/1.46G [00:07<00:04, 138MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  17% 849M/4.97G [00:07<00:35, 117MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  61% 891M/1.46G [00:07<00:04, 116MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  18% 870M/4.97G [00:08<00:32, 126MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 912M/1.46G [00:08<00:04, 121MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  18% 891M/4.97G [00:08<00:31, 130MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 933M/1.46G [00:10<00:18, 28.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 954M/1.46G [00:10<00:13, 37.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  18% 912M/4.97G [00:10<02:33, 26.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  19% 944M/4.97G [00:10<01:39, 40.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  67% 975M/1.46G [00:10<00:11, 43.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  19% 965M/4.97G [00:10<01:20, 49.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 996M/1.46G [00:10<00:08, 55.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  20% 986M/4.97G [00:10<01:03, 62.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  20% 1.01G/4.97G [00:10<00:50, 77.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 1.02G/1.46G [00:11<00:07, 60.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  21% 1.03G/4.97G [00:11<00:44, 87.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 1.04G/1.46G [00:11<00:05, 75.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  21% 1.05G/4.97G [00:11<00:37, 103MB/s] \u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 1.06G/1.46G [00:11<00:04, 93.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  74% 1.08G/1.46G [00:11<00:03, 111MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  22% 1.07G/4.97G [00:11<00:38, 101MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  76% 1.11G/1.46G [00:11<00:02, 143MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  22% 1.09G/4.97G [00:11<00:34, 112MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  22% 1.11G/4.97G [00:12<00:45, 83.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  23% 1.13G/4.97G [00:12<01:17, 49.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 1.13G/1.46G [00:12<00:07, 44.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  23% 1.14G/4.97G [00:12<01:10, 54.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 1.15G/1.46G [00:13<00:05, 54.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  23% 1.16G/4.97G [00:13<00:54, 70.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  80% 1.17G/1.46G [00:13<00:04, 69.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  24% 1.18G/4.97G [00:13<00:43, 87.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 1.20G/1.46G [00:13<00:03, 83.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  24% 1.21G/4.97G [00:13<00:36, 104MB/s] \u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  83% 1.22G/1.46G [00:13<00:02, 96.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  25% 1.23G/4.97G [00:13<00:31, 120MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  85% 1.24G/1.46G [00:13<00:02, 109MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  25% 1.25G/4.97G [00:13<00:27, 136MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  87% 1.27G/1.46G [00:13<00:01, 135MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  26% 1.28G/4.97G [00:13<00:23, 158MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 1.29G/1.46G [00:13<00:01, 132MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  26% 1.30G/4.97G [00:13<00:27, 136MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 1.31G/1.46G [00:14<00:01, 121MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  27% 1.32G/4.97G [00:16<02:21, 25.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 1.33G/1.46G [00:16<00:04, 25.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  27% 1.34G/4.97G [00:16<01:45, 34.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 1.35G/1.46G [00:16<00:03, 34.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  27% 1.36G/4.97G [00:16<01:20, 44.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 1.37G/1.46G [00:16<00:01, 44.0MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  28% 1.38G/4.97G [00:16<01:07, 53.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 1.39G/1.46G [00:16<00:01, 53.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  28% 1.41G/4.97G [00:16<00:55, 64.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 1.42G/1.46G [00:17<00:00, 66.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  29% 1.43G/4.97G [00:17<00:46, 76.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  98% 1.44G/1.46G [00:17<00:00, 77.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  29% 1.45G/4.97G [00:17<00:40, 86.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 1.46G/1.46G [00:17<00:00, 83.7MB/s]\n",
            "\n",
            "\n",
            "model-00001-of-00002.safetensors:  30% 1.47G/4.97G [00:17<00:34, 102MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  30% 1.49G/4.97G [00:17<00:29, 117MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  31% 1.52G/4.97G [00:17<00:23, 144MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  31% 1.54G/4.97G [00:17<00:21, 157MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  32% 1.57G/4.97G [00:17<00:19, 178MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  32% 1.59G/4.97G [00:18<00:18, 179MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  33% 1.61G/4.97G [00:18<00:18, 185MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  33% 1.65G/4.97G [00:18<00:15, 209MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  34% 1.68G/4.97G [00:18<00:15, 219MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  34% 1.71G/4.97G [00:18<00:14, 225MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  35% 1.74G/4.97G [00:18<00:14, 230MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  36% 1.77G/4.97G [00:18<00:13, 231MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  36% 1.80G/4.97G [00:18<00:13, 234MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  37% 1.84G/4.97G [00:19<00:12, 245MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  38% 1.87G/4.97G [00:19<00:12, 249MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  38% 1.90G/4.97G [00:19<00:12, 251MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  39% 1.93G/4.97G [00:19<00:11, 257MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  39% 1.96G/4.97G [00:19<00:12, 250MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  40% 1.99G/4.97G [00:19<00:11, 257MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  41% 2.02G/4.97G [00:19<00:11, 259MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  41% 2.06G/4.97G [00:19<00:11, 257MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  42% 2.09G/4.97G [00:20<00:11, 258MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  43% 2.12G/4.97G [00:20<00:10, 262MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  43% 2.15G/4.97G [00:20<00:10, 269MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  44% 2.18G/4.97G [00:20<00:10, 268MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  45% 2.21G/4.97G [00:20<00:11, 237MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  45% 2.24G/4.97G [00:20<00:11, 245MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  46% 2.28G/4.97G [00:20<00:10, 247MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  46% 2.31G/4.97G [00:20<00:10, 254MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  47% 2.34G/4.97G [00:20<00:09, 263MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  48% 2.37G/4.97G [00:21<00:09, 261MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  48% 2.40G/4.97G [00:21<00:10, 254MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  49% 2.43G/4.97G [00:21<00:15, 168MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  50% 2.46G/4.97G [00:21<00:13, 185MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  50% 2.50G/4.97G [00:21<00:12, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  51% 2.53G/4.97G [00:21<00:11, 218MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  52% 2.56G/4.97G [00:22<00:10, 226MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  52% 2.59G/4.97G [00:22<00:10, 222MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  53% 2.62G/4.97G [00:22<00:10, 233MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  53% 2.65G/4.97G [00:22<00:09, 236MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  54% 2.68G/4.97G [00:22<00:09, 241MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  55% 2.72G/4.97G [00:22<00:09, 246MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  55% 2.75G/4.97G [00:22<00:08, 253MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  56% 2.78G/4.97G [00:23<00:11, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  57% 2.81G/4.97G [00:23<00:10, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  57% 2.84G/4.97G [00:23<00:11, 184MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  58% 2.86G/4.97G [00:23<00:18, 114MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  58% 2.88G/4.97G [00:24<00:17, 119MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  59% 2.92G/4.97G [00:24<00:16, 124MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  59% 2.94G/4.97G [00:24<00:16, 126MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  60% 2.96G/4.97G [00:24<00:15, 131MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  60% 2.98G/4.97G [00:24<00:17, 112MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  60% 3.00G/4.97G [00:25<00:23, 83.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  61% 3.01G/4.97G [00:25<00:23, 81.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  61% 3.03G/4.97G [00:25<00:20, 93.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  61% 3.05G/4.97G [00:25<00:19, 97.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  62% 3.07G/4.97G [00:25<00:17, 110MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  62% 3.09G/4.97G [00:26<00:17, 106MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  63% 3.11G/4.97G [00:26<00:16, 115MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  63% 3.15G/4.97G [00:26<00:15, 121MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  64% 3.17G/4.97G [00:26<00:20, 86.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  64% 3.18G/4.97G [00:27<00:22, 79.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  65% 3.21G/4.97G [00:27<00:17, 97.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  65% 3.24G/4.97G [00:27<00:16, 107MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  66% 3.26G/4.97G [00:27<00:17, 99.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  66% 3.29G/4.97G [00:27<00:13, 123MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  67% 3.32G/4.97G [00:28<00:11, 149MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  68% 3.36G/4.97G [00:30<00:51, 31.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  68% 3.39G/4.97G [00:30<00:36, 43.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  69% 3.42G/4.97G [00:30<00:26, 58.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  69% 3.45G/4.97G [00:31<00:19, 77.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  70% 3.48G/4.97G [00:31<00:15, 98.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  71% 3.51G/4.97G [00:31<00:11, 121MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  71% 3.54G/4.97G [00:31<00:09, 146MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  72% 3.58G/4.97G [00:31<00:11, 123MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  73% 3.61G/4.97G [00:31<00:09, 150MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  73% 3.64G/4.97G [00:31<00:07, 177MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  74% 3.68G/4.97G [00:32<00:06, 213MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  75% 3.71G/4.97G [00:32<00:05, 222MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  75% 3.74G/4.97G [00:32<00:05, 231MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  76% 3.77G/4.97G [00:32<00:04, 247MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  77% 3.81G/4.97G [00:32<00:04, 256MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  77% 3.84G/4.97G [00:32<00:04, 264MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  78% 3.87G/4.97G [00:32<00:04, 263MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  79% 3.90G/4.97G [00:32<00:04, 263MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  79% 3.93G/4.97G [00:33<00:03, 271MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  80% 3.96G/4.97G [00:33<00:03, 273MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  80% 4.00G/4.97G [00:33<00:03, 274MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  81% 4.03G/4.97G [00:33<00:03, 274MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  82% 4.06G/4.97G [00:33<00:03, 267MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  82% 4.09G/4.97G [00:33<00:03, 276MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  83% 4.12G/4.97G [00:33<00:03, 275MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  84% 4.15G/4.97G [00:33<00:03, 254MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  84% 4.18G/4.97G [00:34<00:03, 227MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  85% 4.22G/4.97G [00:34<00:03, 224MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  86% 4.25G/4.97G [00:34<00:03, 229MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  86% 4.28G/4.97G [00:34<00:02, 233MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  87% 4.31G/4.97G [00:34<00:02, 243MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  87% 4.34G/4.97G [00:34<00:02, 236MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  88% 4.37G/4.97G [00:34<00:02, 243MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  89% 4.40G/4.97G [00:34<00:02, 253MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  89% 4.44G/4.97G [00:35<00:02, 236MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  90% 4.47G/4.97G [00:35<00:02, 225MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  91% 4.50G/4.97G [00:35<00:02, 205MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  91% 4.53G/4.97G [00:35<00:02, 208MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  92% 4.56G/4.97G [00:35<00:01, 213MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  92% 4.59G/4.97G [00:35<00:01, 223MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  93% 4.62G/4.97G [00:35<00:01, 233MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  94% 4.66G/4.97G [00:36<00:01, 240MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  94% 4.69G/4.97G [00:36<00:01, 245MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  95% 4.72G/4.97G [00:36<00:00, 255MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  96% 4.76G/4.97G [00:36<00:00, 275MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  96% 4.79G/4.97G [00:36<00:00, 277MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  97% 4.82G/4.97G [00:36<00:00, 286MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  98% 4.85G/4.97G [00:36<00:00, 291MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  98% 4.89G/4.97G [00:36<00:00, 293MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  99% 4.92G/4.97G [00:36<00:00, 295MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors: 100% 4.97G/4.97G [00:37<00:00, 133MB/s]\n",
            "Fetching 2 files: 100% 2/2 [00:37<00:00, 18.84s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [00:30<00:00, 15.12s/it]\n",
            "generation_config.json: 100% 189/189 [00:00<00:00, 1.79MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:679: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "**Understanding Climate Change**\n",
            "\n",
            "Climate change refers to significant, long-term changes in the global climate. The term \"global climate\" encompasses the planet's overall weather patterns, including temperature, precipitation, and wind patterns, over an extended period. Over the past century, human activities, particularly the burning of fossil fuels and deforestation, have significantly contributed to climate change.\n",
            "\n",
            "**Causes of Climate Change**\n",
            "\n",
            "The primary cause of recent climate change is the increase in greenhouse gases in the atmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O), trap heat from the sun, creating a \"greenhouse effect.\" This effect is essential for life on Earth, as it keeps the planet warm enough to support life. However, human activities have intensified this natural process, leading to a warmer climate.\n",
            "\n",
            "**Effects of Climate Change**\n",
            "\n",
            "The effects of climate change are already being felt around the world and are projected to intensify in the coming decades. These effects include:\n",
            "\n",
            "* Rising Temperatures\n",
            "* Heatwaves\n",
            "* Changing Seasons\n",
            "* Melting Ice and Rising Sea Levels\n",
            "* Glacial Retreat\n",
            "* Coastal Erosion\n",
            "* Extreme Weather Events\n",
            "* Ocean Acidification\n",
            "* Coral Reefs\n",
            "* Marine Ecosystems\n",
            "\n",
            "These changes have significant impacts on human societies, ecosystems, and the environment.\n",
            "\n",
            "**References**\n",
            "\n",
            "[DC] Chapter 1: file_path: unknown_source\n",
            "[DC] Chapter 2: file_path: unknown_source\n",
            "[DC] Chapter 3: file_path: unknown_source\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "limit_async: Error in decorated function: CUDA out of memory. Tried to allocate 298.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 30.12 MiB is free. Process 20424 has 14.71 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 580.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/LightRAG/examples/unofficial-sample/lightrag_hf_demo.py\", line 82, in <module>\n",
            "    main()\n",
            "  File \"/content/LightRAG/examples/unofficial-sample/lightrag_hf_demo.py\", line 61, in main\n",
            "    rag.query(\n",
            "  File \"/content/LightRAG/lightrag/lightrag.py\", line 1417, in query\n",
            "    return loop.run_until_complete(self.aquery(query, param, system_prompt))  # type: ignore\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 98, in run_until_complete\n",
            "    return f.result()\n",
            "           ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/asyncio/futures.py\", line 203, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 279, in __step\n",
            "    result = coro.throw(exc)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/content/LightRAG/lightrag/lightrag.py\", line 1443, in aquery\n",
            "    response = await kg_query(\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/content/LightRAG/lightrag/operate.py\", line 961, in kg_query\n",
            "    response = await use_model_func(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/LightRAG/lightrag/utils.py\", line 585, in wait_func\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/asyncio/futures.py\", line 287, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 349, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.11/asyncio/futures.py\", line 203, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/content/LightRAG/lightrag/utils.py\", line 369, in worker\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/LightRAG/lightrag/llm/hf.py\", line 124, in hf_model_complete\n",
            "    result = await hf_model_if_cache(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\", line 400, in <lambda>\n",
            "    self._add_action_func(lambda rs: rs.outcome.result())\n",
            "                                     ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/LightRAG/lightrag/llm/hf.py\", line 109, in hf_model_if_cache\n",
            "    output = hf_model.generate(\n",
            "             ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\", line 2465, in generate\n",
            "    result = self._sample(\n",
            "             ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\", line 3431, in _sample\n",
            "    outputs = self(**model_inputs, return_dict=True)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\", line 965, in wrapper\n",
            "    output = func(self, *args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\", line 821, in forward\n",
            "    outputs: BaseModelOutputWithPast = self.model(\n",
            "                                       ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\", line 965, in wrapper\n",
            "    output = func(self, *args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\", line 571, in forward\n",
            "    layer_outputs = decoder_layer(\n",
            "                    ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\", line 334, in forward\n",
            "    hidden_states = self.mlp(hidden_states)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\", line 172, in forward\n",
            "    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
            "                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 298.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 30.12 MiB is free. Process 20424 has 14.71 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 580.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Blancy/DeepSeek-R1-Distill-Qwen-0.5B-GRPO"
      ],
      "metadata": {
        "id": "e22y-SdPb5q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "شغال"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "TG2Wywbcdny-",
        "outputId": "6e783b85-12f7-4b8c-8bc2-274b6cf3669e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'شغال' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-8b4ac514f617>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mشغال\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'شغال' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from lightrag import LightRAG, QueryParam\n",
        "from lightrag.llm.hf import hf_model_complete, hf_embed\n",
        "from lightrag.utils import EmbeddingFunc\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from lightrag.kg.shared_storage import initialize_pipeline_status\n",
        "\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "WORKING_DIR = \"./dickens\"\n",
        "\n",
        "if not os.path.exists(WORKING_DIR):\n",
        "    os.mkdir(WORKING_DIR)\n",
        "\n",
        "\n",
        "async def initialize_rag():\n",
        "    rag = LightRAG(\n",
        "        working_dir=WORKING_DIR,\n",
        "        llm_model_func=hf_model_complete,\n",
        "        llm_model_name=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "        embedding_func=EmbeddingFunc(\n",
        "            embedding_dim=384,\n",
        "            max_token_size=50,\n",
        "            func=lambda texts: hf_embed(\n",
        "                texts,\n",
        "                tokenizer=AutoTokenizer.from_pretrained(\n",
        "                    \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "                ),\n",
        "                embed_model=AutoModel.from_pretrained(\n",
        "                    \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "                ),\n",
        "            ),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    await rag.initialize_storages()\n",
        "    await initialize_pipeline_status()\n",
        "\n",
        "    return rag\n",
        "\n",
        "\n",
        "def main():\n",
        "    rag = asyncio.run(initialize_rag())\n",
        "\n",
        "    with open(\"/content/LightRAG/examples/unofficial-sample/book.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "        rag.insert(f.read())\n",
        "\n",
        "    # Perform naive search\n",
        "    print(\n",
        "        rag.query(\n",
        "            \"What is Climate Change?\", param=QueryParam(mode=\"naive\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Perform local search\n",
        "    print(\n",
        "        rag.query(\n",
        "            \"What is Climate Change?\", param=QueryParam(mode=\"local\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Perform global search\n",
        "    print(\n",
        "        rag.query(\n",
        "            \"What is Climate Change?\", param=QueryParam(mode=\"global\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Perform hybrid search\n",
        "    print(\n",
        "        rag.query(\n",
        "            \"What is Climate Change?\", param=QueryParam(mode=\"hybrid\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "bLDMp_cddnvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd LightRAG\n",
        "!python /content/LightRAG/examples/unofficial-sample/lightrag_hf_demo.py"
      ],
      "metadata": {
        "id": "K-24I5wXfYZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cr1Z1t-Ufkh0",
        "outputId": "651760e8-92c2-4bbe-8aa2-a9d5d0aa0105"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m109.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bitsandbytes-0.45.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pJeEKz82g-bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### unsloth/Llama-3.2-3B-bnb-4bit"
      ],
      "metadata": {
        "id": "kDB1cEkXg-5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd LightRAG\n",
        "!python /content/LightRAG/examples/unofficial-sample/lightrag_hf_demo.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2ZuupJHgdsJ",
        "outputId": "247854e8-951b-4c4a-dba8-2835beb19828"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'LightRAG'\n",
            "/content/LightRAG\n",
            "\u001b[94m2025-05-22 01:00:53 - pipmaster.package_manager - INFO - Targeting pip associated with Python: /usr/bin/python3 | Command base: /usr/bin/python3 -m pip\u001b[0m\n",
            "INFO: Process 12448 Shared-Data created for Single Process\n",
            "INFO: Process 12448 initialized updated flags for namespace: [full_docs]\n",
            "INFO: Process 12448 ready to initialize storage namespace: [full_docs]\n",
            "INFO: Process 12448 initialized updated flags for namespace: [text_chunks]\n",
            "INFO: Process 12448 ready to initialize storage namespace: [text_chunks]\n",
            "INFO: Process 12448 initialized updated flags for namespace: [entities]\n",
            "INFO: Process 12448 initialized updated flags for namespace: [relationships]\n",
            "INFO: Process 12448 initialized updated flags for namespace: [chunks]\n",
            "INFO: Process 12448 initialized updated flags for namespace: [chunk_entity_relation]\n",
            "INFO: Process 12448 initialized updated flags for namespace: [llm_response_cache]\n",
            "INFO: Process 12448 ready to initialize storage namespace: [llm_response_cache]\n",
            "INFO: Process 12448 initialized updated flags for namespace: [doc_status]\n",
            "INFO: Process 12448 ready to initialize storage namespace: [doc_status]\n",
            "INFO: Process 12448 storage namespace already initialized: [full_docs]\n",
            "INFO: Process 12448 storage namespace already initialized: [text_chunks]\n",
            "INFO: Process 12448 storage namespace already initialized: [llm_response_cache]\n",
            "INFO: Process 12448 storage namespace already initialized: [doc_status]\n",
            "INFO: Process 12448 Pipeline namespace initialized\n",
            "**Understanding Climate Change**\n",
            "\n",
            "Climate change refers to significant, long-term changes in the global climate. The term \"global climate\" encompasses the planet's overall weather patterns, including temperature, precipitation, and wind patterns, over an extended period. Over the past century, human activities, particularly the burning of fossil fuels and deforestation, have significantly contributed to climate change.\n",
            "\n",
            "**Causes of Climate Change**\n",
            "\n",
            "The primary cause of recent climate change is the increase in greenhouse gases in the atmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O), trap heat from the sun, creating a \"greenhouse effect.\" This effect is essential for life on Earth, as it keeps the planet warm enough to support life. However, human activities have intensified this natural process, leading to a warmer climate.\n",
            "\n",
            "**Effects of Climate Change**\n",
            "\n",
            "The effects of climate change are already being felt around the world and are projected to intensify in the coming decades. These effects include:\n",
            "\n",
            "* Rising Temperatures\n",
            "* Heatwaves\n",
            "* Changing Seasons\n",
            "* Melting Ice and Rising Sea Levels\n",
            "* Glacial Retreat\n",
            "* Coastal Erosion\n",
            "* Extreme Weather Events\n",
            "* Ocean Acidification\n",
            "* Coral Reefs\n",
            "* Marine Ecosystems\n",
            "\n",
            "These changes have significant impacts on human societies, ecosystems, and the environment.\n",
            "\n",
            "**References**\n",
            "\n",
            "[DC] Chapter 1: file_path: unknown_source\n",
            "[DC] Chapter 2: file_path: unknown_source\n",
            "[DC] Chapter 3: file_path: unknown_source\n",
            "tokenizer_config.json: 100% 50.6k/50.6k [00:00<00:00, 9.31MB/s]\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "tokenizer.json: 100% 17.2M/17.2M [00:00<00:00, 136MB/s]\n",
            "special_tokens_map.json: 100% 459/459 [00:00<00:00, 3.87MB/s]\n",
            "config.json: 100% 1.51k/1.51k [00:00<00:00, 12.5MB/s]\n",
            "2025-05-22 01:01:07.318996: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747875667.340919   12448 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747875667.345943   12448 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-22 01:01:07.363739: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "model.safetensors: 100% 2.24G/2.24G [00:42<00:00, 52.9MB/s]\n",
            "generation_config.json: 100% 230/230 [00:00<00:00, 1.75MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:679: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n",
            "No JSON-like structure found in the LLM respond.\n",
            "low_level_keywords and high_level_keywords is empty\n",
            "Sorry, I'm not able to provide an answer to that question.[no-context]\n",
            "No JSON-like structure found in the LLM respond.\n",
            "low_level_keywords and high_level_keywords is empty\n",
            "Sorry, I'm not able to provide an answer to that question.[no-context]\n",
            "No JSON-like structure found in the LLM respond.\n",
            "low_level_keywords and high_level_keywords is empty\n",
            "Sorry, I'm not able to provide an answer to that question.[no-context]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "unsloth/Meta-Llama-3.1-8B-bnb-4bit"
      ],
      "metadata": {
        "id": "ZCRUtXkXiLqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd LightRAG\n",
        "!python /content/LightRAG/examples/unofficial-sample/lightrag_hf_demo.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UX4phdNiSYh",
        "outputId": "0bc67e05-7eea-4f9f-ab0b-3b6c3d4d64d3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LightRAG\n",
            "\u001b[94m2025-05-22 01:08:57 - pipmaster.package_manager - INFO - Targeting pip associated with Python: /usr/bin/python3 | Command base: /usr/bin/python3 -m pip\u001b[0m\n",
            "INFO: Process 14470 Shared-Data created for Single Process\n",
            "INFO: Process 14470 initialized updated flags for namespace: [full_docs]\n",
            "INFO: Process 14470 ready to initialize storage namespace: [full_docs]\n",
            "INFO: Process 14470 initialized updated flags for namespace: [text_chunks]\n",
            "INFO: Process 14470 ready to initialize storage namespace: [text_chunks]\n",
            "INFO: Process 14470 initialized updated flags for namespace: [entities]\n",
            "INFO: Process 14470 initialized updated flags for namespace: [relationships]\n",
            "INFO: Process 14470 initialized updated flags for namespace: [chunks]\n",
            "INFO: Process 14470 initialized updated flags for namespace: [chunk_entity_relation]\n",
            "INFO: Process 14470 initialized updated flags for namespace: [llm_response_cache]\n",
            "INFO: Process 14470 ready to initialize storage namespace: [llm_response_cache]\n",
            "INFO: Process 14470 initialized updated flags for namespace: [doc_status]\n",
            "INFO: Process 14470 ready to initialize storage namespace: [doc_status]\n",
            "INFO: Process 14470 storage namespace already initialized: [full_docs]\n",
            "INFO: Process 14470 storage namespace already initialized: [text_chunks]\n",
            "INFO: Process 14470 storage namespace already initialized: [llm_response_cache]\n",
            "INFO: Process 14470 storage namespace already initialized: [doc_status]\n",
            "INFO: Process 14470 Pipeline namespace initialized\n",
            "**Understanding Climate Change**\n",
            "\n",
            "Climate change refers to significant, long-term changes in the global climate. The term \"global climate\" encompasses the planet's overall weather patterns, including temperature, precipitation, and wind patterns, over an extended period. Over the past century, human activities, particularly the burning of fossil fuels and deforestation, have significantly contributed to climate change.\n",
            "\n",
            "**Causes of Climate Change**\n",
            "\n",
            "The primary cause of recent climate change is the increase in greenhouse gases in the atmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O), trap heat from the sun, creating a \"greenhouse effect.\" This effect is essential for life on Earth, as it keeps the planet warm enough to support life. However, human activities have intensified this natural process, leading to a warmer climate.\n",
            "\n",
            "**Effects of Climate Change**\n",
            "\n",
            "The effects of climate change are already being felt around the world and are projected to intensify in the coming decades. These effects include:\n",
            "\n",
            "* Rising Temperatures\n",
            "* Heatwaves\n",
            "* Changing Seasons\n",
            "* Melting Ice and Rising Sea Levels\n",
            "* Glacial Retreat\n",
            "* Coastal Erosion\n",
            "* Extreme Weather Events\n",
            "* Ocean Acidification\n",
            "* Coral Reefs\n",
            "* Marine Ecosystems\n",
            "\n",
            "These changes have significant impacts on human societies, ecosystems, and the environment.\n",
            "\n",
            "**References**\n",
            "\n",
            "[DC] Chapter 1: file_path: unknown_source\n",
            "[DC] Chapter 2: file_path: unknown_source\n",
            "[DC] Chapter 3: file_path: unknown_source\n",
            "tokenizer_config.json: 100% 50.6k/50.6k [00:00<00:00, 6.99MB/s]\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "tokenizer.json: 100% 17.2M/17.2M [00:00<00:00, 117MB/s]\n",
            "special_tokens_map.json: 100% 459/459 [00:00<00:00, 3.70MB/s]\n",
            "config.json: 100% 1.52k/1.52k [00:00<00:00, 9.06MB/s]\n",
            "2025-05-22 01:09:13.163674: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747876153.185041   14470 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747876153.189862   14470 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-22 01:09:13.206067: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "model.safetensors: 100% 5.70G/5.70G [00:37<00:00, 154MB/s]\n",
            "generation_config.json: 100% 235/235 [00:00<00:00, 2.13MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:679: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n",
            "No JSON-like structure found in the LLM respond.\n",
            "low_level_keywords and high_level_keywords is empty\n",
            "Sorry, I'm not able to provide an answer to that question.[no-context]\n",
            "No JSON-like structure found in the LLM respond.\n",
            "low_level_keywords and high_level_keywords is empty\n",
            "Sorry, I'm not able to provide an answer to that question.[no-context]\n",
            "No JSON-like structure found in the LLM respond.\n",
            "low_level_keywords and high_level_keywords is empty\n",
            "Sorry, I'm not able to provide an answer to that question.[no-context]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd LightRAG\n",
        "!python /content/LightRAG/examples/unofficial-sample/lightrag_hf_demo.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9M6BinWzjFfD",
        "outputId": "121f56e0-e643-4314-b1fd-f6f106d6f127"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LightRAG\n",
            "\u001b[94m2025-05-22 01:15:35 - pipmaster.package_manager - INFO - Targeting pip associated with Python: /usr/bin/python3 | Command base: /usr/bin/python3 -m pip\u001b[0m\n",
            "INFO: Process 16211 Shared-Data created for Single Process\n",
            "INFO: Process 16211 initialized updated flags for namespace: [full_docs]\n",
            "INFO: Process 16211 ready to initialize storage namespace: [full_docs]\n",
            "INFO: Process 16211 initialized updated flags for namespace: [text_chunks]\n",
            "INFO: Process 16211 ready to initialize storage namespace: [text_chunks]\n",
            "INFO: Process 16211 initialized updated flags for namespace: [entities]\n",
            "INFO: Process 16211 initialized updated flags for namespace: [relationships]\n",
            "INFO: Process 16211 initialized updated flags for namespace: [chunks]\n",
            "INFO: Process 16211 initialized updated flags for namespace: [chunk_entity_relation]\n",
            "INFO: Process 16211 initialized updated flags for namespace: [llm_response_cache]\n",
            "INFO: Process 16211 ready to initialize storage namespace: [llm_response_cache]\n",
            "INFO: Process 16211 initialized updated flags for namespace: [doc_status]\n",
            "INFO: Process 16211 ready to initialize storage namespace: [doc_status]\n",
            "INFO: Process 16211 storage namespace already initialized: [full_docs]\n",
            "INFO: Process 16211 storage namespace already initialized: [text_chunks]\n",
            "INFO: Process 16211 storage namespace already initialized: [llm_response_cache]\n",
            "INFO: Process 16211 storage namespace already initialized: [doc_status]\n",
            "INFO: Process 16211 Pipeline namespace initialized\n",
            "**Understanding Climate Change**\n",
            "\n",
            "Climate change refers to significant, long-term changes in the global climate. The term \"global climate\" encompasses the planet's overall weather patterns, including temperature, precipitation, and wind patterns, over an extended period. Over the past century, human activities, particularly the burning of fossil fuels and deforestation, have significantly contributed to climate change.\n",
            "\n",
            "**Causes of Climate Change**\n",
            "\n",
            "The primary cause of recent climate change is the increase in greenhouse gases in the atmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O), trap heat from the sun, creating a \"greenhouse effect.\" This effect is essential for life on Earth, as it keeps the planet warm enough to support life. However, human activities have intensified this natural process, leading to a warmer climate.\n",
            "\n",
            "**Effects of Climate Change**\n",
            "\n",
            "The effects of climate change are already being felt around the world and are projected to intensify in the coming decades. These effects include:\n",
            "\n",
            "* Rising Temperatures\n",
            "* Heatwaves\n",
            "* Changing Seasons\n",
            "* Melting Ice and Rising Sea Levels\n",
            "* Glacial Retreat\n",
            "* Coastal Erosion\n",
            "* Extreme Weather Events\n",
            "* Ocean Acidification\n",
            "* Coral Reefs\n",
            "* Marine Ecosystems\n",
            "\n",
            "These changes have significant impacts on human societies, ecosystems, and the environment.\n",
            "\n",
            "**References**\n",
            "\n",
            "[DC] Chapter 1: file_path: unknown_source\n",
            "[DC] Chapter 2: file_path: unknown_source\n",
            "[DC] Chapter 3: file_path: unknown_source\n",
            "2025-05-22 01:15:49.063428: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747876549.099508   16211 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747876549.108541   16211 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-22 01:15:49.136480: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:679: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n",
            "No JSON-like structure found in the LLM respond.\n",
            "low_level_keywords and high_level_keywords is empty\n",
            "Sorry, I'm not able to provide an answer to that question.[no-context]\n",
            "No JSON-like structure found in the LLM respond.\n",
            "low_level_keywords and high_level_keywords is empty\n",
            "Sorry, I'm not able to provide an answer to that question.[no-context]\n",
            "No JSON-like structure found in the LLM respond.\n",
            "low_level_keywords and high_level_keywords is empty\n",
            "Sorry, I'm not able to provide an answer to that question.[no-context]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
      ],
      "metadata": {
        "id": "19Qwm14hnd1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "unsloth/Llama-3.1-8B-Instruct-unsloth-bnb-4bit"
      ],
      "metadata": {
        "id": "LhaNGcrKkBOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd LightRAG\n",
        "!python /content/LightRAG/examples/unofficial-sample/lightrag_hf_demo.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbMC5c49kKvD",
        "outputId": "1cc0aa48-808e-4511-aaa4-992ce0c416b1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LightRAG\n",
            "\u001b[94m2025-05-22 01:17:09 - pipmaster.package_manager - INFO - Targeting pip associated with Python: /usr/bin/python3 | Command base: /usr/bin/python3 -m pip\u001b[0m\n",
            "INFO: Process 16674 Shared-Data created for Single Process\n",
            "INFO: Process 16674 initialized updated flags for namespace: [full_docs]\n",
            "INFO: Process 16674 ready to initialize storage namespace: [full_docs]\n",
            "INFO: Process 16674 initialized updated flags for namespace: [text_chunks]\n",
            "INFO: Process 16674 ready to initialize storage namespace: [text_chunks]\n",
            "INFO: Process 16674 initialized updated flags for namespace: [entities]\n",
            "INFO: Process 16674 initialized updated flags for namespace: [relationships]\n",
            "INFO: Process 16674 initialized updated flags for namespace: [chunks]\n",
            "INFO: Process 16674 initialized updated flags for namespace: [chunk_entity_relation]\n",
            "INFO: Process 16674 initialized updated flags for namespace: [llm_response_cache]\n",
            "INFO: Process 16674 ready to initialize storage namespace: [llm_response_cache]\n",
            "INFO: Process 16674 initialized updated flags for namespace: [doc_status]\n",
            "INFO: Process 16674 ready to initialize storage namespace: [doc_status]\n",
            "INFO: Process 16674 storage namespace already initialized: [full_docs]\n",
            "INFO: Process 16674 storage namespace already initialized: [text_chunks]\n",
            "INFO: Process 16674 storage namespace already initialized: [llm_response_cache]\n",
            "INFO: Process 16674 storage namespace already initialized: [doc_status]\n",
            "INFO: Process 16674 Pipeline namespace initialized\n",
            "**Understanding Climate Change**\n",
            "\n",
            "Climate change refers to significant, long-term changes in the global climate. The term \"global climate\" encompasses the planet's overall weather patterns, including temperature, precipitation, and wind patterns, over an extended period. Over the past century, human activities, particularly the burning of fossil fuels and deforestation, have significantly contributed to climate change.\n",
            "\n",
            "**Causes of Climate Change**\n",
            "\n",
            "The primary cause of recent climate change is the increase in greenhouse gases in the atmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O), trap heat from the sun, creating a \"greenhouse effect.\" This effect is essential for life on Earth, as it keeps the planet warm enough to support life. However, human activities have intensified this natural process, leading to a warmer climate.\n",
            "\n",
            "**Effects of Climate Change**\n",
            "\n",
            "The effects of climate change are already being felt around the world and are projected to intensify in the coming decades. These effects include:\n",
            "\n",
            "* Rising Temperatures\n",
            "* Heatwaves\n",
            "* Changing Seasons\n",
            "* Melting Ice and Rising Sea Levels\n",
            "* Glacial Retreat\n",
            "* Coastal Erosion\n",
            "* Extreme Weather Events\n",
            "* Ocean Acidification\n",
            "* Coral Reefs\n",
            "* Marine Ecosystems\n",
            "\n",
            "These changes have significant impacts on human societies, ecosystems, and the environment.\n",
            "\n",
            "**References**\n",
            "\n",
            "[DC] Chapter 1: file_path: unknown_source\n",
            "[DC] Chapter 2: file_path: unknown_source\n",
            "[DC] Chapter 3: file_path: unknown_source\n",
            "tokenizer_config.json: 100% 55.5k/55.5k [00:00<00:00, 118MB/s]\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "tokenizer.json: 100% 17.2M/17.2M [00:00<00:00, 89.2MB/s]\n",
            "special_tokens_map.json: 100% 454/454 [00:00<00:00, 3.25MB/s]\n",
            "config.json: 100% 1.55k/1.55k [00:00<00:00, 10.9MB/s]\n",
            "2025-05-22 01:17:24.015462: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747876644.037506   16674 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747876644.042430   16674 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-22 01:17:24.058835: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "model.safetensors: 100% 5.96G/5.96G [01:37<00:00, 60.9MB/s]\n",
            "generation_config.json: 100% 239/239 [00:00<00:00, 2.13MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:679: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:679: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n",
            "**Climate Change: An Overview**\n",
            "================================\n",
            "\n",
            "Climate change refers to significant, long-term changes in the global climate. The term \"global climate\" encompasses the planet's overall weather patterns, including temperature, precipitation, and wind patterns, over an extended period. Climate change affects the environment, economies, and societies worldwide.\n",
            "\n",
            "**Causes of Climate Change**\n",
            "---------------------------\n",
            "\n",
            "The primary cause of recent climate change is the increase in greenhouse gases in the atmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O), trap heat from the sun, creating a \"greenhouse effect.\" This effect is essential for life on Earth, as it keeps the planet warm enough to support life. However, human activities have intensified this natural process, leading to a warmer climate. The burning of fossil fuels for energy releases large amounts of CO2, which is a major contributor to climate change.\n",
            "\n",
            "**Effects of Climate Change**\n",
            "---------------------------\n",
            "\n",
            "The effects of climate change are already being felt around the world and are projected to intensify in the coming decades. These effects include rising global temperatures, sea-level rise, and extreme weather events. Climate change affects men and women differently, often exacerbating existing gender inequalities.\n",
            "\n",
            "**References**\n",
            "--------------\n",
            "\n",
            "* [KG] Climate Change (id: 1, file_path: unknown_source)\n",
            "* [KG] Global Climate (id: 33, file_path: unknown_source)\n",
            "* [DC] Chapter 3: Effects of Climate Change (id: 3, file_path: unknown_source)\n",
            "* [KG] Climate Change (id: 9, file_path: unknown_source)\n",
            "* [DC] Chapter 1: Introduction to Climate Change (id: 3, file_path: unknown_source)\n",
            "**Climate Change: A Global Issue**\n",
            "=====================================\n",
            "\n",
            "Climate change refers to significant, long-term changes in the global climate. It is a global issue that affects the environment, economies, and societies. Climate change is primarily caused by human activities that release greenhouse gases, such as carbon dioxide and methane, into the atmosphere, leading to an increase in the average surface temperature of the Earth.\n",
            "\n",
            "**Causes and Effects**\n",
            "------------------------\n",
            "\n",
            "The main causes of climate change include:\n",
            "\n",
            "*   Burning fossil fuels, such as coal, oil, and gas, which release carbon dioxide into the atmosphere.\n",
            "*   Deforestation and land-use changes, which reduce the ability of forests to act as carbon sinks.\n",
            "*   Agriculture, which releases methane and nitrous oxide.\n",
            "*   Industrial processes, which release various greenhouse gases.\n",
            "\n",
            "The effects of climate change include:\n",
            "\n",
            "*   Rising global temperatures, leading to more frequent and severe heatwaves, droughts, and storms.\n",
            "*   Sea-level rise, causing coastal erosion and flooding.\n",
            "*   Changes in precipitation patterns, affecting agriculture and water resources.\n",
            "*   Loss of biodiversity, as species are unable to adapt to changing environmental conditions.\n",
            "\n",
            "**Global Response**\n",
            "--------------------\n",
            "\n",
            "The international community has acknowledged the urgency of addressing climate change. The Paris Agreement, signed in 2015, aims to limit global warming to well below 2°C above pre-industrial levels and pursue efforts to limit the increase to 1.5°C. Countries have submitted nationally determined contributions (NDCs) outlining their climate action plans.\n",
            "\n",
            "**References**\n",
            "---------------\n",
            "\n",
            "*   [KG] unknown_source (Climate Change)\n",
            "*   [DC] unknown_source (Chapter 11: Education and Advocacy)\n",
            "*   [DC] unknown_source (Chapter 14: Climate Change and the Economy)\n",
            "*   [DC] unknown_source (Chapter 15: Climate Change and Technology)\n",
            "*   [DC] unknown_source (Chapter 16: Global Cooperation and Governance)\n",
            "# Climate Change: Understanding the Phenomenon\n",
            "\n",
            "Climate change refers to significant, long-term changes in the global climate. The term \"global climate\" encompasses the planet's overall weather patterns, including temperature, precipitation, and wind patterns, over an extended period.\n",
            "\n",
            "## Causes of Climate Change\n",
            "\n",
            "The primary cause of recent climate change is the increase in greenhouse gases in the atmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O), trap heat from the sun, creating a \"greenhouse effect.\" This effect is essential for life on Earth, as it keeps the planet warm enough to support life. However, human activities have intensified this natural process, leading to a warmer climate.\n",
            "\n",
            "### Human Activities and Climate Change\n",
            "\n",
            "Human activities, particularly the burning of fossil fuels and deforestation, have significantly contributed to climate change. Fossil fuels, such as coal, oil, and natural gas, release large amounts of CO2 when burned, while deforestation reduces the number of trees that can absorb CO2 from the atmosphere.\n",
            "\n",
            "## Effects of Climate Change\n",
            "\n",
            "The effects of climate change are already being felt around the world and are projected to intensify in the coming decades. Rising temperatures, more frequent heatwaves, and changes in precipitation patterns are some of the effects of climate change. Additionally, climate change affects the distribution and prevalence of vector-borne diseases, such as malaria, dengue fever, and Lyme disease.\n",
            "\n",
            "## Importance of Understanding Climate Change\n",
            "\n",
            "Understanding climate change is crucial for taking informed actions to protect our planet for future generations. It requires global cooperation, innovation, and commitment to addressing this pressing challenge.\n",
            "\n",
            "### References\n",
            "\n",
            "* [KG] Chapter 1: Introduction to Climate Change\n",
            "* [KG] Chapter 2: Causes of Climate Change\n",
            "* [DC] Chapter 3: Effects of Climate Change\n",
            "* [KG] Climate Change\n",
            "* [DC] Chapter 1: Introduction to Climate Change\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "هذه المحاولة ناجحة بشكل كبير مقارنة بالمحاولات السابقة!\n",
        "تقييم المحاولة:\n",
        "الإيجابيات الواضحة:\n",
        "الحصول على إجابات ذات معنى: لأول مرة، النموذج اللغوي يُرجع إجابات متماسكة وذات صلة بالسؤال (\"What is Climate Change?\"). هذا يعني أن النموذج قادر على فهم السؤال واستخلاص المعلومات من السياق الذي يوفره book.txt.\n",
        "تجاوز مشكلة CUDA out of memory: اختيار نموذج unsloth/Llama-3.1-8B-Instruct-unsloth-bnb-4bit كان قرارًا صائبًا. نماذج unsloth مُحسَّنة بشكل جيد للتشغيل بكفاءة مع تكميم 4-bit، مما سمح بتشغيل نموذج 8B على الـ GPU المتاح في Colab.\n",
        "تجاوز مشكلة No JSON-like structure و Sorry, I'm not able to provide an answer: يبدو أن هذا النموذج (Instruct version) أكثر قدرة على اتباع التعليمات الضمنية أو التوقعات الخاصة بنظام RAG مثل LightRAG، أو أن LightRAG أقل صرامة في توقعاته مع هذا النموذج.\n",
        "تنوع الإجابات حسب وضع الاستعلام:\n",
        "الإجابة الأولى (التي تبدو كأنها من وضع naive) قدمت ملخصًا جيدًا يتضمن الأسباب والآثار.\n",
        "الإجابة الثانية (ربما local) أيضًا جيدة وتضيف تفاصيل حول الاستجابة العالمية واتفاقية باريس.\n",
        "الإجابة الثالثة (ربما global أو hybrid) أيضًا متماسكة وتكرر بعض النقاط الهامة مع إضافة تأثير الأمراض المنقولة بالنواقل.\n",
        "تضمين المراجع: المخرجات تتضمن قسم \"References\" مع إشارات إلى [KG] (Knowledge Graph) و [DC] (Document Chunks). هذا يدل على أن نظام LightRAG يعمل كما هو مصمم، حيث يسترجع معلومات من مصادر مختلفة (أو أنواع مختلفة من المعالجة) ويدمجها.\n",
        "زيادة max_token_size إلى 500: هذا تغيير جيد. max_token_size=50 كان منخفضًا جدًا لنموذج التضمين. 500 قيمة أكثر منطقية لـ sentence-transformers/all-MiniLM-L6-v2، مما يسمح بتضمين قطع نصية (chunks) أكبر وأكثر معنى.\n",
        "نقاط قد تحتاج إلى تحسين أو ملاحظات:\n",
        "تكرار بعض المعلومات: من الطبيعي في أنظمة RAG أن يكون هناك بعض التكرار بين الإجابات الناتجة عن أوضاع استعلام مختلفة، خاصة إذا كانت جميعها تستقي من نفس المستند الأساسي.\n",
        "مراجع file_path: unknown_source: هذا يشير إلى أن LightRAG لم يتمكن من تحديد مسار الملف الأصلي للمعلومات المسترجعة من الـ Knowledge Graph أو Document Chunks. قد يكون هذا بسبب كيفية إدراج البيانات (rag.insert(f.read()) يمرر المحتوى كسلسلة نصية واحدة) أو تكوين داخلي في LightRAG يتوقع بيانات وصفية إضافية للملفات. في هذا المثال البسيط، قد لا يكون هذا مشكلة كبيرة، ولكن في تطبيقات أكثر تعقيدًا، قد ترغب في تتبع مصدر المعلومات بدقة.\n",
        "التحذيرات المستمرة (Xet Storage, TensorFlow/XLA, cuFFT, etc.): كما ذكرنا سابقًا، هذه تحذيرات تتعلق بالبيئة أو التنزيل وليست أخطاء تمنع تشغيل الكود. يمكن تجاهلها طالما أن النتائج النهائية جيدة.\n",
        "الخلاصة:\n",
        "هذه المحاولة هي الأفضل حتى الآن وتمثل تقدمًا كبيرًا. لقد نجحت في:\n",
        "اختيار نموذج لغوي مناسب ومُحسَّن يعمل ضمن قيود الذاكرة.\n",
        "جعل نظام RAG ينتج إجابات مفيدة وذات صلة.\n",
        "التغلب على المشاكل التقنية الرئيسية التي واجهتك سابقًا.\n",
        "الخطوات التالية المحتملة (إذا أردت المزيد من التحسين):\n",
        "تجربة أسئلة أكثر تعقيدًا أو تحديدًا: لاختبار مدى قوة النظام.\n",
        "استكشاف بارامترات QueryParam: مثل top_k، أو أي بارامترات أخرى للتحكم في عملية الاسترجاع والتوليد.\n",
        "التعمق في فهم LightRAG: إذا كنت ستستخدم هذه المكتبة بشكل مكثف، فإن فهم كيفية عمل الـ \"prompts\" الداخلية، وكيفية بناء الـ Knowledge Graph، وكيفية تخصيص السلوك سيكون مفيدًا.\n",
        "تقييم جودة الإجابات: قارن الإجابات مع المعلومات الفعلية في book.txt لتقييم مدى دقتها واكتمالها.\n",
        "بشكل عام، نتيجة ممتازة! لقد أظهرت صبرًا ومثابرة في حل المشكلات.\n"
      ],
      "metadata": {
        "id": "SnvtnWWKnBX0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال"
      ],
      "metadata": {
        "id": "s1RHeYPvnaeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "محاولة ناجحة\n",
        "/content/LightRAG/examples/unofficial-sample/lightrag_hf_demo.py\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "from lightrag import LightRAG, QueryParam\n",
        "from lightrag.llm.hf import hf_model_complete, hf_embed\n",
        "from lightrag.utils import EmbeddingFunc\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from lightrag.kg.shared_storage import initialize_pipeline_status\n",
        "\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "WORKING_DIR = \"./dickens\"\n",
        "\n",
        "if not os.path.exists(WORKING_DIR):\n",
        "    os.mkdir(WORKING_DIR)\n",
        "\n",
        "\n",
        "async def initialize_rag():\n",
        "    rag = LightRAG(\n",
        "        working_dir=WORKING_DIR,\n",
        "        llm_model_func=hf_model_complete,\n",
        "        llm_model_name=\"unsloth/Llama-3.1-8B-Instruct-unsloth-bnb-4bit\",\n",
        "        embedding_func=EmbeddingFunc(\n",
        "            embedding_dim=384,\n",
        "            max_token_size=500,\n",
        "            func=lambda texts: hf_embed(\n",
        "                texts,\n",
        "                tokenizer=AutoTokenizer.from_pretrained(\n",
        "                    \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "                ),\n",
        "                embed_model=AutoModel.from_pretrained(\n",
        "                    \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "                ),\n",
        "            ),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    await rag.initialize_storages()\n",
        "    await initialize_pipeline_status()\n",
        "\n",
        "    return rag\n",
        "\n",
        "\n",
        "def main():\n",
        "    rag = asyncio.run(initialize_rag())\n",
        "\n",
        "    with open(\"/content/LightRAG/examples/unofficial-sample/book.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "        rag.insert(f.read())\n",
        "\n",
        "    # Perform naive search\n",
        "    print(\n",
        "        rag.query(\n",
        "            \"What is Climate Change?\", param=QueryParam(mode=\"naive\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Perform local search\n",
        "    print(\n",
        "        rag.query(\n",
        "            \"What is Climate Change?\", param=QueryParam(mode=\"local\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Perform global search\n",
        "    print(\n",
        "        rag.query(\n",
        "            \"What is Climate Change?\", param=QueryParam(mode=\"global\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Perform hybrid search\n",
        "    print(\n",
        "        rag.query(\n",
        "            \"What is Climate Change?\", param=QueryParam(mode=\"hybrid\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "/content/LightRAG/lightrag/llm/hf.py\n",
        "\n",
        "\n",
        "import copy\n",
        "import os\n",
        "from functools import lru_cache\n",
        "\n",
        "import pipmaster as pm  # Pipmaster for dynamic library install\n",
        "\n",
        "# install specific modules\n",
        "if not pm.is_installed(\"transformers\"):\n",
        "    pm.install(\"transformers\")\n",
        "if not pm.is_installed(\"torch\"):\n",
        "    pm.install(\"torch\")\n",
        "if not pm.is_installed(\"numpy\"):\n",
        "    pm.install(\"numpy\")\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tenacity import (\n",
        "    retry,\n",
        "    stop_after_attempt,\n",
        "    wait_exponential,\n",
        "    retry_if_exception_type,\n",
        ")\n",
        "from lightrag.exceptions import (\n",
        "    APIConnectionError,\n",
        "    RateLimitError,\n",
        "    APITimeoutError,\n",
        ")\n",
        "from lightrag.utils import (\n",
        "    locate_json_string_body_from_string,\n",
        ")\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "\n",
        "@lru_cache(maxsize=1)\n",
        "def initialize_hf_model(model_name):\n",
        "    hf_tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_name, device_map=\"auto\", trust_remote_code=True\n",
        "    )\n",
        "    hf_model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name, device_map=\"auto\", trust_remote_code=True\n",
        "    )\n",
        "    if hf_tokenizer.pad_token is None:\n",
        "        hf_tokenizer.pad_token = hf_tokenizer.eos_token\n",
        "\n",
        "    return hf_model, hf_tokenizer\n",
        "\n",
        "\n",
        "@retry(\n",
        "    stop=stop_after_attempt(3),\n",
        "    wait=wait_exponential(multiplier=1, min=4, max=10),\n",
        "    retry=retry_if_exception_type(\n",
        "        (RateLimitError, APIConnectionError, APITimeoutError)\n",
        "    ),\n",
        ")\n",
        "async def hf_model_if_cache(\n",
        "    model,\n",
        "    prompt,\n",
        "    system_prompt=None,\n",
        "    history_messages=[],\n",
        "    **kwargs,\n",
        ") -> str:\n",
        "    model_name = model\n",
        "    hf_model, hf_tokenizer = initialize_hf_model(model_name)\n",
        "    messages = []\n",
        "    if system_prompt:\n",
        "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
        "    messages.extend(history_messages)\n",
        "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    kwargs.pop(\"hashing_kv\", None)\n",
        "    input_prompt = \"\"\n",
        "    try:\n",
        "        input_prompt = hf_tokenizer.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "    except Exception:\n",
        "        try:\n",
        "            ori_message = copy.deepcopy(messages)\n",
        "            if messages[0][\"role\"] == \"system\":\n",
        "                messages[1][\"content\"] = (\n",
        "                    \"<system>\"\n",
        "                    + messages[0][\"content\"]\n",
        "                    + \"</system>\\n\"\n",
        "                    + messages[1][\"content\"]\n",
        "                )\n",
        "                messages = messages[1:]\n",
        "                input_prompt = hf_tokenizer.apply_chat_template(\n",
        "                    messages, tokenize=False, add_generation_prompt=True\n",
        "                )\n",
        "        except Exception:\n",
        "            len_message = len(ori_message)\n",
        "            for msgid in range(len_message):\n",
        "                input_prompt = (\n",
        "                    input_prompt\n",
        "                    + \"<\"\n",
        "                    + ori_message[msgid][\"role\"]\n",
        "                    + \">\"\n",
        "                    + ori_message[msgid][\"content\"]\n",
        "                    + \"</\"\n",
        "                    + ori_message[msgid][\"role\"]\n",
        "                    + \">\\n\"\n",
        "                )\n",
        "\n",
        "    input_ids = hf_tokenizer(\n",
        "        input_prompt, return_tensors=\"pt\", padding=True, truncation=True\n",
        "    ).to(\"cuda\")\n",
        "    inputs = {k: v.to(hf_model.device) for k, v in input_ids.items()}\n",
        "    output = hf_model.generate(\n",
        "        **input_ids, max_new_tokens=512, num_return_sequences=1, early_stopping=True\n",
        "    )\n",
        "    response_text = hf_tokenizer.decode(\n",
        "        output[0][len(inputs[\"input_ids\"][0]) :], skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    return response_text\n",
        "\n",
        "\n",
        "async def hf_model_complete(\n",
        "    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n",
        ") -> str:\n",
        "    keyword_extraction = kwargs.pop(\"keyword_extraction\", None)\n",
        "    model_name = kwargs[\"hashing_kv\"].global_config[\"llm_model_name\"]\n",
        "    result = await hf_model_if_cache(\n",
        "        model_name,\n",
        "        prompt,\n",
        "        system_prompt=system_prompt,\n",
        "        history_messages=history_messages,\n",
        "        **kwargs,\n",
        "    )\n",
        "    if keyword_extraction:  # TODO: use JSON API\n",
        "        return locate_json_string_body_from_string(result)\n",
        "    return result\n",
        "\n",
        "\n",
        "async def hf_embed(texts: list[str], tokenizer, embed_model) -> np.ndarray:\n",
        "    # Detect the appropriate device\n",
        "    if torch.cuda.is_available():\n",
        "        device = next(embed_model.parameters()).device  # Use CUDA if available\n",
        "    elif torch.backends.mps.is_available():\n",
        "        device = torch.device(\"mps\")  # Use MPS for Apple Silicon\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")  # Fallback to CPU\n",
        "\n",
        "    # Move the model to the detected device\n",
        "    embed_model = embed_model.to(device)\n",
        "\n",
        "    # Tokenize the input texts and move them to the same device\n",
        "    encoded_texts = tokenizer(\n",
        "        texts, return_tensors=\"pt\", padding=True, truncation=True\n",
        "    ).to(device)\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        outputs = embed_model(\n",
        "            input_ids=encoded_texts[\"input_ids\"],\n",
        "            attention_mask=encoded_texts[\"attention_mask\"],\n",
        "        )\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "    # Convert embeddings to NumPy\n",
        "    if embeddings.dtype == torch.bfloat16:\n",
        "        return embeddings.detach().to(torch.float32).cpu().numpy()\n",
        "    else:\n",
        "        return embeddings.detach().cpu().numpy()\n",
        "\n"
      ],
      "metadata": {
        "id": "oxif06UZnG1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
      ],
      "metadata": {
        "id": "D0wSwJhzngrU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gkfb-BosnBOO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$"
      ],
      "metadata": {
        "id": "FsNgfHLjn_6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%cd /content/LightRAG\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "from lightrag import LightRAG, QueryParam\n",
        "from lightrag.llm.hf import hf_model_complete, hf_embed\n",
        "from lightrag.utils import EmbeddingFunc\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from lightrag.kg.shared_storage import initialize_pipeline_status\n",
        "\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "WORKING_DIR = \"./dickens\"\n",
        "\n",
        "if not os.path.exists(WORKING_DIR):\n",
        "    os.mkdir(WORKING_DIR)\n",
        "\n",
        "\n",
        "async def initialize_rag():\n",
        "    rag = LightRAG(\n",
        "        working_dir=WORKING_DIR,\n",
        "        llm_model_func=hf_model_complete,\n",
        "        llm_model_name=\"unsloth/Llama-3.1-8B-Instruct-unsloth-bnb-4bit\",\n",
        "        embedding_func=EmbeddingFunc(\n",
        "            embedding_dim=384,\n",
        "            max_token_size=500,\n",
        "            func=lambda texts: hf_embed(\n",
        "                texts,\n",
        "                tokenizer=AutoTokenizer.from_pretrained(\n",
        "                    \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "                ),\n",
        "                embed_model=AutoModel.from_pretrained(\n",
        "                    \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "                ),\n",
        "            ),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    await rag.initialize_storages()\n",
        "    await initialize_pipeline_status()\n",
        "\n",
        "    return rag\n",
        "\n",
        "\n",
        "def main():\n",
        "    rag = asyncio.run(initialize_rag())\n",
        "\n",
        "    with open(\"/content/LightRAG/examples/unofficial-sample/book.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "        rag.insert(f.read())\n",
        "\n",
        "    # Perform naive search\n",
        "    print(\n",
        "        rag.query(\n",
        "            \"What is Climate Change?\", param=QueryParam(mode=\"naive\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Perform local search\n",
        "    print(\n",
        "        rag.query(\n",
        "            \"What is Climate Change?\", param=QueryParam(mode=\"local\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Perform global search\n",
        "    print(\n",
        "        rag.query(\n",
        "            \"What is Climate Change?\", param=QueryParam(mode=\"global\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Perform hybrid search\n",
        "    print(\n",
        "        rag.query(\n",
        "            \"What is Climate Change?\", param=QueryParam(mode=\"hybrid\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d81Sr7kPnBL7",
        "outputId": "e04e742a-f068-4604-a8c4-44025e7eb30d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LightRAG\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[94m2025-05-22 01:33:12 - pipmaster.package_manager - INFO - Targeting pip associated with Python: /usr/bin/python3 | Command base: /usr/bin/python3 -m pip\u001b[0m\n",
            "INFO: Process 20736 Shared-Data created for Single Process\n",
            "INFO: Process 20736 initialized updated flags for namespace: [full_docs]\n",
            "INFO: Process 20736 ready to initialize storage namespace: [full_docs]\n",
            "INFO: Process 20736 initialized updated flags for namespace: [text_chunks]\n",
            "INFO: Process 20736 ready to initialize storage namespace: [text_chunks]\n",
            "INFO: Process 20736 initialized updated flags for namespace: [entities]\n",
            "INFO: Process 20736 initialized updated flags for namespace: [relationships]\n",
            "INFO: Process 20736 initialized updated flags for namespace: [chunks]\n",
            "INFO: Process 20736 initialized updated flags for namespace: [chunk_entity_relation]\n",
            "INFO: Process 20736 initialized updated flags for namespace: [llm_response_cache]\n",
            "INFO: Process 20736 ready to initialize storage namespace: [llm_response_cache]\n",
            "INFO: Process 20736 initialized updated flags for namespace: [doc_status]\n",
            "INFO: Process 20736 ready to initialize storage namespace: [doc_status]\n",
            "INFO: Process 20736 storage namespace already initialized: [full_docs]\n",
            "INFO: Process 20736 storage namespace already initialized: [text_chunks]\n",
            "INFO: Process 20736 storage namespace already initialized: [llm_response_cache]\n",
            "INFO: Process 20736 storage namespace already initialized: [doc_status]\n",
            "INFO: Process 20736 Pipeline namespace initialized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Understanding Climate Change**\n",
            "\n",
            "Climate change refers to significant, long-term changes in the global climate. The term \"global climate\" encompasses the planet's overall weather patterns, including temperature, precipitation, and wind patterns, over an extended period. Over the past century, human activities, particularly the burning of fossil fuels and deforestation, have significantly contributed to climate change.\n",
            "\n",
            "**Causes of Climate Change**\n",
            "\n",
            "The primary cause of recent climate change is the increase in greenhouse gases in the atmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O), trap heat from the sun, creating a \"greenhouse effect.\" This effect is essential for life on Earth, as it keeps the planet warm enough to support life. However, human activities have intensified this natural process, leading to a warmer climate.\n",
            "\n",
            "**Effects of Climate Change**\n",
            "\n",
            "The effects of climate change are already being felt around the world and are projected to intensify in the coming decades. These effects include:\n",
            "\n",
            "* Rising Temperatures\n",
            "* Heatwaves\n",
            "* Changing Seasons\n",
            "* Melting Ice and Rising Sea Levels\n",
            "* Glacial Retreat\n",
            "* Coastal Erosion\n",
            "* Extreme Weather Events\n",
            "* Ocean Acidification\n",
            "* Coral Reefs\n",
            "* Marine Ecosystems\n",
            "\n",
            "These changes have significant impacts on human societies, ecosystems, and the environment.\n",
            "\n",
            "**References**\n",
            "\n",
            "[DC] Chapter 1: file_path: unknown_source\n",
            "[DC] Chapter 2: file_path: unknown_source\n",
            "[DC] Chapter 3: file_path: unknown_source\n",
            "**Climate Change: An Overview**\n",
            "================================\n",
            "\n",
            "Climate change refers to significant, long-term changes in the global climate. The term \"global climate\" encompasses the planet's overall weather patterns, including temperature, precipitation, and wind patterns, over an extended period. Climate change affects the environment, economies, and societies worldwide.\n",
            "\n",
            "**Causes of Climate Change**\n",
            "---------------------------\n",
            "\n",
            "The primary cause of recent climate change is the increase in greenhouse gases in the atmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O), trap heat from the sun, creating a \"greenhouse effect.\" This effect is essential for life on Earth, as it keeps the planet warm enough to support life. However, human activities have intensified this natural process, leading to a warmer climate. The burning of fossil fuels for energy releases large amounts of CO2, which is a major contributor to climate change.\n",
            "\n",
            "**Effects of Climate Change**\n",
            "---------------------------\n",
            "\n",
            "The effects of climate change are already being felt around the world and are projected to intensify in the coming decades. These effects include rising global temperatures, sea-level rise, and extreme weather events. Climate change affects men and women differently, often exacerbating existing gender inequalities.\n",
            "\n",
            "**References**\n",
            "--------------\n",
            "\n",
            "* [KG] Climate Change (id: 1, file_path: unknown_source)\n",
            "* [KG] Global Climate (id: 33, file_path: unknown_source)\n",
            "* [DC] Chapter 3: Effects of Climate Change (id: 3, file_path: unknown_source)\n",
            "* [KG] Climate Change (id: 9, file_path: unknown_source)\n",
            "* [DC] Chapter 1: Introduction to Climate Change (id: 3, file_path: unknown_source)\n",
            "**Climate Change: A Global Issue**\n",
            "=====================================\n",
            "\n",
            "Climate change refers to significant, long-term changes in the global climate. It is a global issue that affects the environment, economies, and societies. Climate change is primarily caused by human activities that release greenhouse gases, such as carbon dioxide and methane, into the atmosphere, leading to an increase in the average surface temperature of the Earth.\n",
            "\n",
            "**Causes and Effects**\n",
            "------------------------\n",
            "\n",
            "The main causes of climate change include:\n",
            "\n",
            "*   Burning fossil fuels, such as coal, oil, and gas, which release carbon dioxide into the atmosphere.\n",
            "*   Deforestation and land-use changes, which reduce the ability of forests to act as carbon sinks.\n",
            "*   Agriculture, which releases methane and nitrous oxide.\n",
            "*   Industrial processes, which release various greenhouse gases.\n",
            "\n",
            "The effects of climate change include:\n",
            "\n",
            "*   Rising global temperatures, leading to more frequent and severe heatwaves, droughts, and storms.\n",
            "*   Sea-level rise, causing coastal erosion and flooding.\n",
            "*   Changes in precipitation patterns, affecting agriculture and water resources.\n",
            "*   Loss of biodiversity, as species are unable to adapt to changing environmental conditions.\n",
            "\n",
            "**Global Response**\n",
            "--------------------\n",
            "\n",
            "The international community has acknowledged the urgency of addressing climate change. The Paris Agreement, signed in 2015, aims to limit global warming to well below 2°C above pre-industrial levels and pursue efforts to limit the increase to 1.5°C. Countries have submitted nationally determined contributions (NDCs) outlining their climate action plans.\n",
            "\n",
            "**References**\n",
            "---------------\n",
            "\n",
            "*   [KG] unknown_source (Climate Change)\n",
            "*   [DC] unknown_source (Chapter 11: Education and Advocacy)\n",
            "*   [DC] unknown_source (Chapter 14: Climate Change and the Economy)\n",
            "*   [DC] unknown_source (Chapter 15: Climate Change and Technology)\n",
            "*   [DC] unknown_source (Chapter 16: Global Cooperation and Governance)\n",
            "# Climate Change: Understanding the Phenomenon\n",
            "\n",
            "Climate change refers to significant, long-term changes in the global climate. The term \"global climate\" encompasses the planet's overall weather patterns, including temperature, precipitation, and wind patterns, over an extended period.\n",
            "\n",
            "## Causes of Climate Change\n",
            "\n",
            "The primary cause of recent climate change is the increase in greenhouse gases in the atmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O), trap heat from the sun, creating a \"greenhouse effect.\" This effect is essential for life on Earth, as it keeps the planet warm enough to support life. However, human activities have intensified this natural process, leading to a warmer climate.\n",
            "\n",
            "### Human Activities and Climate Change\n",
            "\n",
            "Human activities, particularly the burning of fossil fuels and deforestation, have significantly contributed to climate change. Fossil fuels, such as coal, oil, and natural gas, release large amounts of CO2 when burned, while deforestation reduces the number of trees that can absorb CO2 from the atmosphere.\n",
            "\n",
            "## Effects of Climate Change\n",
            "\n",
            "The effects of climate change are already being felt around the world and are projected to intensify in the coming decades. Rising temperatures, more frequent heatwaves, and changes in precipitation patterns are some of the effects of climate change. Additionally, climate change affects the distribution and prevalence of vector-borne diseases, such as malaria, dengue fever, and Lyme disease.\n",
            "\n",
            "## Importance of Understanding Climate Change\n",
            "\n",
            "Understanding climate change is crucial for taking informed actions to protect our planet for future generations. It requires global cooperation, innovation, and commitment to addressing this pressing challenge.\n",
            "\n",
            "### References\n",
            "\n",
            "* [KG] Chapter 1: Introduction to Climate Change\n",
            "* [KG] Chapter 2: Causes of Climate Change\n",
            "* [DC] Chapter 3: Effects of Climate Change\n",
            "* [KG] Climate Change\n",
            "* [DC] Chapter 1: Introduction to Climate Change\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
      ],
      "metadata": {
        "id": "on4u343en9Pj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cBKV1qb0nBJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P-0do_SvoF_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-yDz_zdIoGD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3alhdKbcoGHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "luZshKFboGMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VPwKSGqjoGQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VTTQJaNDoGVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
      ],
      "metadata": {
        "id": "slD3bAD5rFNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%cd /content/LightRAG\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "from lightrag import LightRAG, QueryParam\n",
        "from lightrag.llm.hf import hf_model_complete, hf_embed\n",
        "from lightrag.utils import EmbeddingFunc\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from lightrag.kg.shared_storage import initialize_pipeline_status\n",
        "\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "WORKING_DIR = \"./dickens\"\n",
        "\n",
        "if not os.path.exists(WORKING_DIR):\n",
        "    os.mkdir(WORKING_DIR)\n",
        "\n",
        "\n",
        "async def initialize_rag():\n",
        "    rag = LightRAG(\n",
        "        working_dir=WORKING_DIR,\n",
        "        llm_model_func=hf_model_complete,\n",
        "        llm_model_name=\"unsloth/Llama-3.1-8B-Instruct-unsloth-bnb-4bit\",\n",
        "        embedding_func=EmbeddingFunc(\n",
        "            embedding_dim=384,\n",
        "            max_token_size=500,\n",
        "            func=lambda texts: hf_embed(\n",
        "                texts,\n",
        "                tokenizer=AutoTokenizer.from_pretrained(\n",
        "                    \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "                ),\n",
        "                embed_model=AutoModel.from_pretrained(\n",
        "                    \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "                ),\n",
        "            ),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    await rag.initialize_storages()\n",
        "    await initialize_pipeline_status()\n",
        "\n",
        "    return rag\n",
        "\n",
        "\n",
        "def main():\n",
        "    rag = asyncio.run(initialize_rag())\n",
        "\n",
        "    with open(\"/content/LightRAG/examples/unofficial-sample/book.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "        rag.insert(f.read())\n",
        "\n",
        "    # Perform naive search\n",
        "    print(\n",
        "        rag.query(\n",
        "            \"What are the effects of climate change?\", param=QueryParam(mode=\"naive\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Perform local search\n",
        "    print(\n",
        "        rag.query(\n",
        "            \"What are the effects of climate change?\", param=QueryParam(mode=\"local\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Perform global search\n",
        "    print(\n",
        "        rag.query(\n",
        "            \"What are the effects of climate change?\", param=QueryParam(mode=\"global\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Perform hybrid search\n",
        "    print(\n",
        "        rag.query(\n",
        "            \"What are the effects of climate change?\", param=QueryParam(mode=\"hybrid\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suD2f5GzoGZE",
        "outputId": "7e598783-0b5c-4925-9cac-600f7d4ea5eb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[94m2025-05-22 01:38:35 - pipmaster.package_manager - INFO - Targeting pip associated with Python: /usr/bin/python3 | Command base: /usr/bin/python3 -m pip\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LightRAG\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: Process 22129 Shared-Data created for Single Process\n",
            "INFO: Process 22129 initialized updated flags for namespace: [full_docs]\n",
            "INFO: Process 22129 ready to initialize storage namespace: [full_docs]\n",
            "INFO: Process 22129 initialized updated flags for namespace: [text_chunks]\n",
            "INFO: Process 22129 ready to initialize storage namespace: [text_chunks]\n",
            "INFO: Process 22129 initialized updated flags for namespace: [entities]\n",
            "INFO: Process 22129 initialized updated flags for namespace: [relationships]\n",
            "INFO: Process 22129 initialized updated flags for namespace: [chunks]\n",
            "INFO: Process 22129 initialized updated flags for namespace: [chunk_entity_relation]\n",
            "INFO: Process 22129 initialized updated flags for namespace: [llm_response_cache]\n",
            "INFO: Process 22129 ready to initialize storage namespace: [llm_response_cache]\n",
            "INFO: Process 22129 initialized updated flags for namespace: [doc_status]\n",
            "INFO: Process 22129 ready to initialize storage namespace: [doc_status]\n",
            "INFO: Process 22129 storage namespace already initialized: [full_docs]\n",
            "INFO: Process 22129 storage namespace already initialized: [text_chunks]\n",
            "INFO: Process 22129 storage namespace already initialized: [llm_response_cache]\n",
            "INFO: Process 22129 storage namespace already initialized: [doc_status]\n",
            "INFO: Process 22129 Pipeline namespace initialized\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:679: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Effects of Climate Change**\n",
            "==========================\n",
            "\n",
            "Climate change has far-reaching and devastating impacts on the environment, human health, and the economy. The effects of climate change are already being felt around the world and are projected to intensify in the coming decades. Some of the key effects of climate change include:\n",
            "\n",
            "### Rising Temperatures\n",
            "\n",
            "Global temperatures have risen by about 1.2 degrees Celsius (2.2 degrees Fahrenheit) since the late 19th century. This warming is not uniform, with some regions experiencing more significant increases than others.\n",
            "\n",
            "### Heatwaves\n",
            "\n",
            "Heatwaves are becoming more frequent and severe, posing risks to human health, agriculture, and infrastructure. Cities are particularly vulnerable due to the \"urban heat island\" effect.\n",
            "\n",
            "### Changing Seasons\n",
            "\n",
            "Climate change is altering the timing and length of seasons, affecting ecosystems and human activities. For example, spring is arriving earlier, and winters are becoming shorter and milder in many regions.\n",
            "\n",
            "### Melting Ice and Rising Sea Levels\n",
            "\n",
            "Warmer temperatures are causing polar ice caps and glaciers to melt, contributing to rising sea levels. Sea levels have risen by about 20 centimeters (8 inches) in the past century, threatening coastal communities and ecosystems.\n",
            "\n",
            "### Extreme Weather Events\n",
            "\n",
            "Climate change is linked to an increase in the frequency and severity of extreme weather events, such as hurricanes, heatwaves, droughts, and heavy rainfall. These events can have devastating impacts on communities, economies, and ecosystems.\n",
            "\n",
            "### Ocean Acidification\n",
            "\n",
            "Increased CO2 levels in the atmosphere lead to higher concentrations of CO2 in the oceans, causing the water to become more acidic. This can harm marine life, particularly organisms with calcium carbonate shells or skeletons, such as corals and some shellfish.\n",
            "\n",
            "### Biodiversity Loss\n",
            "\n",
            "Climate change affects the distribution and prevalence of vector-borne diseases, such as malaria, dengue fever, and Lyme disease. Warmer temperatures and changing precipitation patterns can expand the habitats of disease-carrying insects, increasing the risk of outbreaks.\n",
            "\n",
            "### Food and Water Security\n",
            "\n",
            "Climate change affects food production and water availability, impacting nutrition and health. Droughts, floods, and changing growing seasons can reduce crop yields and disrupt food supply chains.\n",
            "\n",
            "These are just a few of the many effects of climate change. It is essential to address this issue through mitigation and adaptation efforts to minimize its impacts on the environment, human health, and the economy.\n",
            "\n",
            "**References**\n",
            "\n",
            "[DC 1] unknown_source\n",
            "[DC 2] unknown_source\n",
            "[DC 3] unknown_source\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No JSON-like structure found in the LLM respond.\n",
            "low_level_keywords and high_level_keywords is empty\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry, I'm not able to provide an answer to that question.[no-context]\n",
            "**Climate Change Effects: Understanding the Consequences**\n",
            "\n",
            "Climate change has far-reaching and devastating effects on the environment, economies, and societies worldwide. The consequences of climate change are multifaceted and interconnected, making it a pressing global issue that requires immediate attention.\n",
            "\n",
            "**Environmental Impacts**\n",
            "\n",
            "Climate change affects the environment in various ways, including:\n",
            "\n",
            "*   **Rising Sea Levels**: Thawing of polar ice caps and glaciers leads to sea-level rise, causing coastal erosion, flooding, and saltwater intrusion into freshwater sources.\n",
            "*   **Extreme Weather Events**: Increased frequency and intensity of heatwaves, droughts, and storms, which can lead to loss of life, property damage, and displacement of communities.\n",
            "*   **Biodiversity Loss**: Climate change alters ecosystems, leading to the decline or extinction of species, disrupting food chains, and compromising ecosystem services.\n",
            "*   **Water Scarcity**: Changes in precipitation patterns and increased evaporation due to warmer temperatures lead to droughts, affecting agriculture, industry, and human consumption.\n",
            "\n",
            "**Social and Economic Consequences**\n",
            "\n",
            "Climate change has significant social and economic implications, including:\n",
            "\n",
            "*   **Food Insecurity**: Climate-related disruptions to agriculture and food systems can lead to food shortages, price increases, and malnutrition.\n",
            "*   **Economic Losses**: Climate-related disasters can cause significant economic losses, including damage to infrastructure, loss of property, and impacts on tourism and trade.\n",
            "*   **Human Migration**: Climate change can lead to displacement of communities, particularly in vulnerable regions, straining social services and resources.\n",
            "*   **Health Impacts**: Climate change exacerbates the spread of diseases, heat-related illnesses, and other health problems, particularly among vulnerable populations.\n",
            "\n",
            "**References**\n",
            "\n",
            "*   [KG] Chapter 10: Climate Change and Human Health\n",
            "*   [DC] Chapter 10: Climate Change and Human Health\n",
            "*   [KG] Chapter 11: Education and Advocacy\n",
            "*   [DC] Chapter 11: Education and Advocacy\n",
            "*   [KG] Chapter 14: Climate Change and the Economy\n",
            "**Effects of Climate Change**\n",
            "\n",
            "Climate change has far-reaching impacts on the environment, economies, and societies. The effects of climate change are already being felt around the world and are projected to intensify in the coming decades.\n",
            "\n",
            "**Rising Temperatures**\n",
            "\n",
            "Global temperatures have risen by about 1.2 degrees Celsius (2.2 degrees Fahrenheit) since the late 19th century. This warming is not uniform, with some regions experiencing more significant temperature increases than others.\n",
            "\n",
            "**Sea-Level Rise and Coastal Flooding**\n",
            "\n",
            "Rising sea levels are causing coastal erosion, flooding, and saltwater intrusion into freshwater sources. This affects not only coastal ecosystems but also human settlements, agriculture, and infrastructure.\n",
            "\n",
            "**Extreme Weather Events**\n",
            "\n",
            "Climate change is linked to an increase in extreme weather events such as heatwaves, droughts, and heavy rainfall. These events have devastating impacts on ecosystems, economies, and human populations.\n",
            "\n",
            "**Impacts on Biodiversity**\n",
            "\n",
            "Climate change is altering ecosystems, leading to the loss of biodiversity. This includes the extinction of species, disruption of food chains, and degradation of habitats.\n",
            "\n",
            "**Human Health Impacts**\n",
            "\n",
            "Climate change affects human health in various ways, including:\n",
            "\n",
            "*   Heat-related illnesses\n",
            "*   Vector-borne diseases\n",
            "*   Respiratory and cardiovascular diseases\n",
            "*   Food and water security\n",
            "\n",
            "**Economic Impacts**\n",
            "\n",
            "Climate change has significant economic implications, including:\n",
            "\n",
            "*   Damage to infrastructure and property\n",
            "*   Loss of productivity and income\n",
            "*   Increased healthcare costs\n",
            "*   Disruption of supply chains and trade\n",
            "\n",
            "**Social Impacts**\n",
            "\n",
            "Climate change affects human societies in various ways, including:\n",
            "\n",
            "*   Displacement and migration\n",
            "*   Social inequality and injustice\n",
            "*   Cultural and traditional practices\n",
            "\n",
            "**References**\n",
            "\n",
            "*   [KG] Chapter 3: Effects of Climate Change\n",
            "*   [DC] Chapter 3: Effects of Climate Change\n",
            "*   [KG] Climate Change\n",
            "*   [DC] Chapter 1: Introduction to Climate Change\n",
            "*   [KG] Deforestation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "هذه المحاولة تواصل النجاح الذي رأيناه في المحاولة السابقة، مع بعض الملاحظات الإضافية.\n",
        "تقييم المحاولة:\n",
        "الإيجابيات:\n",
        "استمرار النجاح في توليد إجابات ذات معنى: تم تغيير السؤال إلى \"What are the effects of climate change?\" (ما هي آثار تغير المناخ؟)، والنموذج مرة أخرى قدم إجابات مفصلة وذات صلة.\n",
        "تغطية جوانب متعددة للآثار: الإجابات تغطي نطاقًا واسعًا من الآثار، بما في ذلك ارتفاع درجات الحرارة، موجات الحر، تغير الفصول، ذوبان الجليد، الظواهر الجوية المتطرفة، تحمض المحيطات، فقدان التنوع البيولوجي، وتأثيرات على الأمن الغذائي والمائي.\n",
        "بنية جيدة للإجابات: الإجابات منظمة بشكل جيد باستخدام العناوين والقوائم النقطية، مما يجعلها سهلة القراءة والفهم.\n",
        "استخدام المراجع: لا يزال النظام يدرج المراجع من [DC] (Document Chunks) و [KG] (Knowledge Graph)، مما يشير إلى أن آلية الاسترجاع تعمل.\n",
        "عدم وجود أخطاء فادحة: لم تحدث أخطاء CUDA out of memory أو انهيارات أخرى.\n",
        "الملاحظات والتحديات:\n",
        "ظهور No JSON-like structure found و Sorry, I'm not able to provide an answer مرة أخرى (جزئيًا):\n",
        "من المثير للاهتمام أن الإجابة الأولى (التي تبدو من وضع naive) كانت ممتازة ومفصلة.\n",
        "ولكن، بعد الإجابة الأولى، ظهرت رسائل الخطأ/الاعتذار:\n",
        "No JSON-like structure found in the LLM respond.\n",
        "low_level_keywords and high_level_keywords is empty\n",
        "Sorry, I'm not able to provide an answer to that question.[no-context]\n",
        "Use code with caution.\n",
        "ثم، الإجابة الثالثة (التي تبدو من وضع global أو hybrid، بناءً على المراجع مثل \"Chapter 10: Climate Change and Human Health\") كانت جيدة ومفصلة أيضًا، وإن كانت مختلفة قليلاً في التركيز عن الأولى.\n",
        "الاستنتاج المحتمل: يبدو أن وضع استعلام معين (الذي أدى إلى ظهور رسائل الخطأ في المنتصف، ربما وضع local) يواجه صعوبة في الحصول على استجابة بالتنسيق المتوقع أو استجابة مفيدة من النموذج اللغوي، بينما الأوضاع الأخرى تعمل بشكل جيد. هذا يشير إلى أن كيفية بناء الـ \"prompt\" أو معالجة الاستجابة قد تختلف بين أوضاع QueryParam المختلفة في LightRAG.\n",
        "مراجع unknown_source: لا تزال بعض المراجع تشير إلى unknown_source، خاصةً مع [DC 1], [DC 2], [DC 3] في الإجابة الأولى. هذا يعزز فكرة أن البيانات الوصفية لمسار الملف قد لا يتم تمريرها أو الاحتفاظ بها بشكل كامل عند استخدام rag.insert(f.read()).\n",
        "تحذير HF_TOKEN:\n",
        "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning:\n",
        "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
        "Use code with caution.\n",
        "هذا تحذير من huggingface_hub يفيد بأنك لم تقم بتكوين توكن Hugging Face في أسرار Colab. هذا ليس ضروريًا للنماذج العامة، ولكنه قد يكون مطلوبًا للنماذج الخاصة أو لبعض الميزات المتقدمة (مثل رفع النماذج أو استخدام واجهات برمجة تطبيقات معينة تتطلب مصادقة). لا يؤثر هذا على قدرتك على تنزيل وتشغيل النماذج العامة مثل الذي تستخدمه.\n",
        "الخلاصة والتقييم العام:\n",
        "المحاولة لا تزال ناجحة بشكل كبير في جوهرها، حيث أن النظام قادر على الإجابة على الأسئلة بشكل جيد باستخدام نموذج مُكمَّم.\n",
        "التحدي الرئيسي الذي ظهر مجددًا (وإن كان بشكل متقطع) هو مشكلة No JSON-like structure found / Sorry, I'm not able to provide an answer، والتي يبدو أنها مرتبطة بوضع استعلام معين داخل LightRAG.\n",
        "توصيات:\n",
        "التحقيق في أوضاع QueryParam:\n",
        "جرب تشغيل كل وضع استعلام (naive, local, global, hybrid) بشكل منفصل بدلاً من تشغيلها جميعًا بالتتابع. هذا سيساعدك على تحديد الوضع الذي يسبب المشكلة بدقة.\n",
        "مثال:\n",
        "# ... (بعد rag.insert)\n",
        "print(\"Testing NAIVE mode:\")\n",
        "print(rag.query(\"What are the effects of climate change?\", param=QueryParam(mode=\"naive\")))\n",
        "\n",
        "print(\"\\nTesting LOCAL mode:\")\n",
        "print(rag.query(\"What are the effects of climate change?\", param=QueryParam(mode=\"local\")))\n",
        "# وهكذا لباقي الأوضاع\n",
        "Use code with caution.\n",
        "Python\n",
        "إذا كان وضع local (أو أي وضع آخر) هو الذي يفشل باستمرار، فقد تحتاج إلى النظر في كيفية عمل هذا الوضع تحديدًا في LightRAG أو تجنب استخدامه إذا كانت الأوضاع الأخرى كافية لاحتياجاتك.\n",
        "مراجعة وثائق LightRAG (إذا كانت متاحة): ابحث عن أي معلومات حول كيفية عمل أوضاع الاستعلام المختلفة، وما إذا كانت تتطلب تكوينات أو \"prompts\" معينة للنماذج المختلفة.\n",
        "التجربة مع top_k في الوضع الإشكالي: إذا كان وضع local هو المشكلة، جرب تعديل top_k (عدد المستندات المسترجعة) له. قد يكون السياق الذي يتم بناؤه كبيرًا جدًا أو صغيرًا جدًا أو غير مناسب للنموذج في هذا الوضع.\n",
        "print(rag.query(\"What are the effects of climate change?\", param=QueryParam(mode=\"local\", top_k=3))) # جرب قيم مختلفة لـ top_k\n",
        "Use code with caution.\n",
        "Python\n",
        "بشكل عام، أنت على المسار الصحيح. تحديد الوضع الإشكالي في QueryParam هو الخطوة التالية المنطقية لاستكشاف هذه المشكلة المتقطع"
      ],
      "metadata": {
        "id": "uiTJQ1uqrAmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
      ],
      "metadata": {
        "id": "QT6E5DXGrCTu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hUN8d4xirBd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m_kJSK1rnBGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s93HOPpinBDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NSd-2-pXnBAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "هذه المحاولة تواصل النجاح الذي رأيناه في المحاولة السابقة، مع بعض الملاحظات الإضافية.\n",
        "تقييم المحاولة:\n",
        "الإيجابيات:\n",
        "استمرار النجاح في توليد إجابات ذات معنى: تم تغيير السؤال إلى \"What are the effects of climate change?\" (ما هي آثار تغير المناخ؟)، والنموذج مرة أخرى قدم إجابات مفصلة وذات صلة.\n",
        "تغطية جوانب متعددة للآثار: الإجابات تغطي نطاقًا واسعًا من الآثار، بما في ذلك ارتفاع درجات الحرارة، موجات الحر، تغير الفصول، ذوبان الجليد، الظواهر الجوية المتطرفة، تحمض المحيطات، فقدان التنوع البيولوجي، وتأثيرات على الأمن الغذائي والمائي.\n",
        "بنية جيدة للإجابات: الإجابات منظمة بشكل جيد باستخدام العناوين والقوائم النقطية، مما يجعلها سهلة القراءة والفهم.\n",
        "استخدام المراجع: لا يزال النظام يدرج المراجع من [DC] (Document Chunks) و [KG] (Knowledge Graph)، مما يشير إلى أن آلية الاسترجاع تعمل.\n",
        "عدم وجود أخطاء فادحة: لم تحدث أخطاء CUDA out of memory أو انهيارات أخرى.\n",
        "الملاحظات والتحديات:\n",
        "ظهور No JSON-like structure found و Sorry, I'm not able to provide an answer مرة أخرى (جزئيًا):\n",
        "من المثير للاهتمام أن الإجابة الأولى (التي تبدو من وضع naive) كانت ممتازة ومفصلة.\n",
        "ولكن، بعد الإجابة الأولى، ظهرت رسائل الخطأ/الاعتذار:\n",
        "No JSON-like structure found in the LLM respond.\n",
        "low_level_keywords and high_level_keywords is empty\n",
        "Sorry, I'm not able to provide an answer to that question.[no-context]\n",
        "Use code with caution.\n",
        "ثم، الإجابة الثالثة (التي تبدو من وضع global أو hybrid، بناءً على المراجع مثل \"Chapter 10: Climate Change and Human Health\") كانت جيدة ومفصلة أيضًا، وإن كانت مختلفة قليلاً في التركيز عن الأولى.\n",
        "الاستنتاج المحتمل: يبدو أن وضع استعلام معين (الذي أدى إلى ظهور رسائل الخطأ في المنتصف، ربما وضع local) يواجه صعوبة في الحصول على استجابة بالتنسيق المتوقع أو استجابة مفيدة من النموذج اللغوي، بينما الأوضاع الأخرى تعمل بشكل جيد. هذا يشير إلى أن كيفية بناء الـ \"prompt\" أو معالجة الاستجابة قد تختلف بين أوضاع QueryParam المختلفة في LightRAG.\n",
        "مراجع unknown_source: لا تزال بعض المراجع تشير إلى unknown_source، خاصةً مع [DC 1], [DC 2], [DC 3] في الإجابة الأولى. هذا يعزز فكرة أن البيانات الوصفية لمسار الملف قد لا يتم تمريرها أو الاحتفاظ بها بشكل كامل عند استخدام rag.insert(f.read()).\n",
        "تحذير HF_TOKEN:\n",
        "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning:\n",
        "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
        "Use code with caution.\n",
        "هذا تحذير من huggingface_hub يفيد بأنك لم تقم بتكوين توكن Hugging Face في أسرار Colab. هذا ليس ضروريًا للنماذج العامة، ولكنه قد يكون مطلوبًا للنماذج الخاصة أو لبعض الميزات المتقدمة (مثل رفع النماذج أو استخدام واجهات برمجة تطبيقات معينة تتطلب مصادقة). لا يؤثر هذا على قدرتك على تنزيل وتشغيل النماذج العامة مثل الذي تستخدمه.\n",
        "الخلاصة والتقييم العام:\n",
        "المحاولة لا تزال ناجحة بشكل كبير في جوهرها، حيث أن النظام قادر على الإجابة على الأسئلة بشكل جيد باستخدام نموذج مُكمَّم.\n",
        "التحدي الرئيسي الذي ظهر مجددًا (وإن كان بشكل متقطع) هو مشكلة No JSON-like structure found / Sorry, I'm not able to provide an answer، والتي يبدو أنها مرتبطة بوضع استعلام معين داخل LightRAG.\n",
        "توصيات:\n",
        "التحقيق في أوضاع QueryParam:\n",
        "جرب تشغيل كل وضع استعلام (naive, local, global, hybrid) بشكل منفصل بدلاً من تشغيلها جميعًا بالتتابع. هذا سيساعدك على تحديد الوضع الذي يسبب المشكلة بدقة.\n",
        "مثال:\n",
        "# ... (بعد rag.insert)\n",
        "print(\"Testing NAIVE mode:\")\n",
        "print(rag.query(\"What are the effects of climate change?\", param=QueryParam(mode=\"naive\")))\n",
        "\n",
        "print(\"\\nTesting LOCAL mode:\")\n",
        "print(rag.query(\"What are the effects of climate change?\", param=QueryParam(mode=\"local\")))\n",
        "# وهكذا لباقي الأوضاع\n",
        "Use code with caution.\n",
        "Python\n",
        "إذا كان وضع local (أو أي وضع آخر) هو الذي يفشل باستمرار، فقد تحتاج إلى النظر في كيفية عمل هذا الوضع تحديدًا في LightRAG أو تجنب استخدامه إذا كانت الأوضاع الأخرى كافية لاحتياجاتك.\n",
        "مراجعة وثائق LightRAG (إذا كانت متاحة): ابحث عن أي معلومات حول كيفية عمل أوضاع الاستعلام المختلفة، وما إذا كانت تتطلب تكوينات أو \"prompts\" معينة للنماذج المختلفة.\n",
        "التجربة مع top_k في الوضع الإشكالي: إذا كان وضع local هو المشكلة، جرب تعديل top_k (عدد المستندات المسترجعة) له. قد يكون السياق الذي يتم بناؤه كبيرًا جدًا أو صغيرًا جدًا أو غير مناسب للنموذج في هذا الوضع.\n",
        "print(rag.query(\"What are the effects of climate change?\", param=QueryParam(mode=\"local\", top_k=3))) # جرب قيم مختلفة لـ top_k\n",
        "Use code with caution.\n",
        "Python\n",
        "بشكل عام، أنت على المسار الصحيح. تحديد الوضع الإشكالي في QueryParam هو الخطوة التالية المنطقية لاستكشاف هذه المشكلة المتقطع"
      ],
      "metadata": {
        "id": "lu-19SfdnA80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "unsloth/Phi-4-mini-instruct"
      ],
      "metadata": {
        "id": "iL8r1upAlFXM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Blancy/DeepSeek-R1-Distill-Qwen-0.5B-GRPO"
      ],
      "metadata": {
        "id": "ADSvHej-lJ7b"
      }
    }
  ]
}