# -*- coding: utf-8 -*-
"""suc_LightRAG_GPU.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16cGRtWNVq3NUcfF34SAaBJ-IUPrQBXPE
"""







!git clone https://github.com/HKUDS/LightRAG.git

# Commented out IPython magic to ensure Python compatibility.
# %cd LightRAG
!pip install -e .

!huggingface-cli login --token XXXXXXXXXXXXX

# Commented out IPython magic to ensure Python compatibility.
# %cd LightRAG
!python /content/LightRAG/examples/unofficial-sample/lightrag_hf_demo.py

Blancy/DeepSeek-R1-Distill-Qwen-0.5B-GRPO

شغال

import os

from lightrag import LightRAG, QueryParam
from lightrag.llm.hf import hf_model_complete, hf_embed
from lightrag.utils import EmbeddingFunc
from transformers import AutoModel, AutoTokenizer
from lightrag.kg.shared_storage import initialize_pipeline_status

import asyncio
import nest_asyncio

nest_asyncio.apply()

WORKING_DIR = "./dickens"

if not os.path.exists(WORKING_DIR):
    os.mkdir(WORKING_DIR)


async def initialize_rag():
    rag = LightRAG(
        working_dir=WORKING_DIR,
        llm_model_func=hf_model_complete,
        llm_model_name="meta-llama/Llama-3.2-3B-Instruct",
        embedding_func=EmbeddingFunc(
            embedding_dim=384,
            max_token_size=50,
            func=lambda texts: hf_embed(
                texts,
                tokenizer=AutoTokenizer.from_pretrained(
                    "sentence-transformers/all-MiniLM-L6-v2"
                ),
                embed_model=AutoModel.from_pretrained(
                    "sentence-transformers/all-MiniLM-L6-v2"
                ),
            ),
        ),
    )

    await rag.initialize_storages()
    await initialize_pipeline_status()

    return rag


def main():
    rag = asyncio.run(initialize_rag())

    with open("/content/LightRAG/examples/unofficial-sample/book.txt", "r", encoding="utf-8") as f:
        rag.insert(f.read())

    # Perform naive search
    print(
        rag.query(
            "What is Climate Change?", param=QueryParam(mode="naive")
        )
    )

    # Perform local search
    print(
        rag.query(
            "What is Climate Change?", param=QueryParam(mode="local")
        )
    )

    # Perform global search
    print(
        rag.query(
            "What is Climate Change?", param=QueryParam(mode="global")
        )
    )

    # Perform hybrid search
    print(
        rag.query(
            "What is Climate Change?", param=QueryParam(mode="hybrid")
        )
    )


if __name__ == "__main__":
    main()

# Commented out IPython magic to ensure Python compatibility.
# %cd LightRAG
!python /content/LightRAG/examples/unofficial-sample/lightrag_hf_demo.py

!pip install bitsandbytes



"""### unsloth/Llama-3.2-3B-bnb-4bit"""

# Commented out IPython magic to ensure Python compatibility.
# %cd LightRAG
!python /content/LightRAG/examples/unofficial-sample/lightrag_hf_demo.py

"""unsloth/Meta-Llama-3.1-8B-bnb-4bit"""

# Commented out IPython magic to ensure Python compatibility.
# %cd LightRAG
!python /content/LightRAG/examples/unofficial-sample/lightrag_hf_demo.py

# Commented out IPython magic to ensure Python compatibility.
# %cd LightRAG
!python /content/LightRAG/examples/unofficial-sample/lightrag_hf_demo.py

"""شغال%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

unsloth/Llama-3.1-8B-Instruct-unsloth-bnb-4bit
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd LightRAG
!python /content/LightRAG/examples/unofficial-sample/lightrag_hf_demo.py

"""هذه المحاولة ناجحة بشكل كبير مقارنة بالمحاولات السابقة!
تقييم المحاولة:
الإيجابيات الواضحة:
الحصول على إجابات ذات معنى: لأول مرة، النموذج اللغوي يُرجع إجابات متماسكة وذات صلة بالسؤال ("What is Climate Change?"). هذا يعني أن النموذج قادر على فهم السؤال واستخلاص المعلومات من السياق الذي يوفره book.txt.
تجاوز مشكلة CUDA out of memory: اختيار نموذج unsloth/Llama-3.1-8B-Instruct-unsloth-bnb-4bit كان قرارًا صائبًا. نماذج unsloth مُحسَّنة بشكل جيد للتشغيل بكفاءة مع تكميم 4-bit، مما سمح بتشغيل نموذج 8B على الـ GPU المتاح في Colab.
تجاوز مشكلة No JSON-like structure و Sorry, I'm not able to provide an answer: يبدو أن هذا النموذج (Instruct version) أكثر قدرة على اتباع التعليمات الضمنية أو التوقعات الخاصة بنظام RAG مثل LightRAG، أو أن LightRAG أقل صرامة في توقعاته مع هذا النموذج.
تنوع الإجابات حسب وضع الاستعلام:
الإجابة الأولى (التي تبدو كأنها من وضع naive) قدمت ملخصًا جيدًا يتضمن الأسباب والآثار.
الإجابة الثانية (ربما local) أيضًا جيدة وتضيف تفاصيل حول الاستجابة العالمية واتفاقية باريس.
الإجابة الثالثة (ربما global أو hybrid) أيضًا متماسكة وتكرر بعض النقاط الهامة مع إضافة تأثير الأمراض المنقولة بالنواقل.
تضمين المراجع: المخرجات تتضمن قسم "References" مع إشارات إلى [KG] (Knowledge Graph) و [DC] (Document Chunks). هذا يدل على أن نظام LightRAG يعمل كما هو مصمم، حيث يسترجع معلومات من مصادر مختلفة (أو أنواع مختلفة من المعالجة) ويدمجها.
زيادة max_token_size إلى 500: هذا تغيير جيد. max_token_size=50 كان منخفضًا جدًا لنموذج التضمين. 500 قيمة أكثر منطقية لـ sentence-transformers/all-MiniLM-L6-v2، مما يسمح بتضمين قطع نصية (chunks) أكبر وأكثر معنى.
نقاط قد تحتاج إلى تحسين أو ملاحظات:
تكرار بعض المعلومات: من الطبيعي في أنظمة RAG أن يكون هناك بعض التكرار بين الإجابات الناتجة عن أوضاع استعلام مختلفة، خاصة إذا كانت جميعها تستقي من نفس المستند الأساسي.
مراجع file_path: unknown_source: هذا يشير إلى أن LightRAG لم يتمكن من تحديد مسار الملف الأصلي للمعلومات المسترجعة من الـ Knowledge Graph أو Document Chunks. قد يكون هذا بسبب كيفية إدراج البيانات (rag.insert(f.read()) يمرر المحتوى كسلسلة نصية واحدة) أو تكوين داخلي في LightRAG يتوقع بيانات وصفية إضافية للملفات. في هذا المثال البسيط، قد لا يكون هذا مشكلة كبيرة، ولكن في تطبيقات أكثر تعقيدًا، قد ترغب في تتبع مصدر المعلومات بدقة.
التحذيرات المستمرة (Xet Storage, TensorFlow/XLA, cuFFT, etc.): كما ذكرنا سابقًا، هذه تحذيرات تتعلق بالبيئة أو التنزيل وليست أخطاء تمنع تشغيل الكود. يمكن تجاهلها طالما أن النتائج النهائية جيدة.
الخلاصة:
هذه المحاولة هي الأفضل حتى الآن وتمثل تقدمًا كبيرًا. لقد نجحت في:
اختيار نموذج لغوي مناسب ومُحسَّن يعمل ضمن قيود الذاكرة.
جعل نظام RAG ينتج إجابات مفيدة وذات صلة.
التغلب على المشاكل التقنية الرئيسية التي واجهتك سابقًا.
الخطوات التالية المحتملة (إذا أردت المزيد من التحسين):
تجربة أسئلة أكثر تعقيدًا أو تحديدًا: لاختبار مدى قوة النظام.
استكشاف بارامترات QueryParam: مثل top_k، أو أي بارامترات أخرى للتحكم في عملية الاسترجاع والتوليد.
التعمق في فهم LightRAG: إذا كنت ستستخدم هذه المكتبة بشكل مكثف، فإن فهم كيفية عمل الـ "prompts" الداخلية، وكيفية بناء الـ Knowledge Graph، وكيفية تخصيص السلوك سيكون مفيدًا.
تقييم جودة الإجابات: قارن الإجابات مع المعلومات الفعلية في book.txt لتقييم مدى دقتها واكتمالها.
بشكل عام، نتيجة ممتازة! لقد أظهرت صبرًا ومثابرة في حل المشكلات.

شغال
"""

محاولة ناجحة
/content/LightRAG/examples/unofficial-sample/lightrag_hf_demo.py


import os

from lightrag import LightRAG, QueryParam
from lightrag.llm.hf import hf_model_complete, hf_embed
from lightrag.utils import EmbeddingFunc
from transformers import AutoModel, AutoTokenizer
from lightrag.kg.shared_storage import initialize_pipeline_status

import asyncio
import nest_asyncio

nest_asyncio.apply()

WORKING_DIR = "./dickens"

if not os.path.exists(WORKING_DIR):
    os.mkdir(WORKING_DIR)


async def initialize_rag():
    rag = LightRAG(
        working_dir=WORKING_DIR,
        llm_model_func=hf_model_complete,
        llm_model_name="unsloth/Llama-3.1-8B-Instruct-unsloth-bnb-4bit",
        embedding_func=EmbeddingFunc(
            embedding_dim=384,
            max_token_size=500,
            func=lambda texts: hf_embed(
                texts,
                tokenizer=AutoTokenizer.from_pretrained(
                    "sentence-transformers/all-MiniLM-L6-v2"
                ),
                embed_model=AutoModel.from_pretrained(
                    "sentence-transformers/all-MiniLM-L6-v2"
                ),
            ),
        ),
    )

    await rag.initialize_storages()
    await initialize_pipeline_status()

    return rag


def main():
    rag = asyncio.run(initialize_rag())

    with open("/content/LightRAG/examples/unofficial-sample/book.txt", "r", encoding="utf-8") as f:
        rag.insert(f.read())

    # Perform naive search
    print(
        rag.query(
            "What is Climate Change?", param=QueryParam(mode="naive")
        )
    )

    # Perform local search
    print(
        rag.query(
            "What is Climate Change?", param=QueryParam(mode="local")
        )
    )

    # Perform global search
    print(
        rag.query(
            "What is Climate Change?", param=QueryParam(mode="global")
        )
    )

    # Perform hybrid search
    print(
        rag.query(
            "What is Climate Change?", param=QueryParam(mode="hybrid")
        )
    )


if __name__ == "__main__":
    main()





/content/LightRAG/lightrag/llm/hf.py


import copy
import os
from functools import lru_cache

import pipmaster as pm  # Pipmaster for dynamic library install

# install specific modules
if not pm.is_installed("transformers"):
    pm.install("transformers")
if not pm.is_installed("torch"):
    pm.install("torch")
if not pm.is_installed("numpy"):
    pm.install("numpy")

from transformers import AutoTokenizer, AutoModelForCausalLM
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
)
from lightrag.exceptions import (
    APIConnectionError,
    RateLimitError,
    APITimeoutError,
)
from lightrag.utils import (
    locate_json_string_body_from_string,
)
import torch
import numpy as np

os.environ["TOKENIZERS_PARALLELISM"] = "false"


@lru_cache(maxsize=1)
def initialize_hf_model(model_name):
    hf_tokenizer = AutoTokenizer.from_pretrained(
        model_name, device_map="auto", trust_remote_code=True
    )
    hf_model = AutoModelForCausalLM.from_pretrained(
        model_name, device_map="auto", trust_remote_code=True
    )
    if hf_tokenizer.pad_token is None:
        hf_tokenizer.pad_token = hf_tokenizer.eos_token

    return hf_model, hf_tokenizer


@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10),
    retry=retry_if_exception_type(
        (RateLimitError, APIConnectionError, APITimeoutError)
    ),
)
async def hf_model_if_cache(
    model,
    prompt,
    system_prompt=None,
    history_messages=[],
    **kwargs,
) -> str:
    model_name = model
    hf_model, hf_tokenizer = initialize_hf_model(model_name)
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.extend(history_messages)
    messages.append({"role": "user", "content": prompt})
    kwargs.pop("hashing_kv", None)
    input_prompt = ""
    try:
        input_prompt = hf_tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
    except Exception:
        try:
            ori_message = copy.deepcopy(messages)
            if messages[0]["role"] == "system":
                messages[1]["content"] = (
                    "<system>"
                    + messages[0]["content"]
                    + "</system>\n"
                    + messages[1]["content"]
                )
                messages = messages[1:]
                input_prompt = hf_tokenizer.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=True
                )
        except Exception:
            len_message = len(ori_message)
            for msgid in range(len_message):
                input_prompt = (
                    input_prompt
                    + "<"
                    + ori_message[msgid]["role"]
                    + ">"
                    + ori_message[msgid]["content"]
                    + "</"
                    + ori_message[msgid]["role"]
                    + ">\n"
                )

    input_ids = hf_tokenizer(
        input_prompt, return_tensors="pt", padding=True, truncation=True
    ).to("cuda")
    inputs = {k: v.to(hf_model.device) for k, v in input_ids.items()}
    output = hf_model.generate(
        **input_ids, max_new_tokens=512, num_return_sequences=1, early_stopping=True
    )
    response_text = hf_tokenizer.decode(
        output[0][len(inputs["input_ids"][0]) :], skip_special_tokens=True
    )

    return response_text


async def hf_model_complete(
    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs
) -> str:
    keyword_extraction = kwargs.pop("keyword_extraction", None)
    model_name = kwargs["hashing_kv"].global_config["llm_model_name"]
    result = await hf_model_if_cache(
        model_name,
        prompt,
        system_prompt=system_prompt,
        history_messages=history_messages,
        **kwargs,
    )
    if keyword_extraction:  # TODO: use JSON API
        return locate_json_string_body_from_string(result)
    return result


async def hf_embed(texts: list[str], tokenizer, embed_model) -> np.ndarray:
    # Detect the appropriate device
    if torch.cuda.is_available():
        device = next(embed_model.parameters()).device  # Use CUDA if available
    elif torch.backends.mps.is_available():
        device = torch.device("mps")  # Use MPS for Apple Silicon
    else:
        device = torch.device("cpu")  # Fallback to CPU

    # Move the model to the detected device
    embed_model = embed_model.to(device)

    # Tokenize the input texts and move them to the same device
    encoded_texts = tokenizer(
        texts, return_tensors="pt", padding=True, truncation=True
    ).to(device)

    # Perform inference
    with torch.no_grad():
        outputs = embed_model(
            input_ids=encoded_texts["input_ids"],
            attention_mask=encoded_texts["attention_mask"],
        )
        embeddings = outputs.last_hidden_state.mean(dim=1)

    # Convert embeddings to NumPy
    if embeddings.dtype == torch.bfloat16:
        return embeddings.detach().to(torch.float32).cpu().numpy()
    else:
        return embeddings.detach().cpu().numpy()

"""%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"""



"""شغال$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$"""

# Commented out IPython magic to ensure Python compatibility.

# %cd /content/LightRAG



import os

from lightrag import LightRAG, QueryParam
from lightrag.llm.hf import hf_model_complete, hf_embed
from lightrag.utils import EmbeddingFunc
from transformers import AutoModel, AutoTokenizer
from lightrag.kg.shared_storage import initialize_pipeline_status

import asyncio
import nest_asyncio

nest_asyncio.apply()

WORKING_DIR = "./dickens"

if not os.path.exists(WORKING_DIR):
    os.mkdir(WORKING_DIR)


async def initialize_rag():
    rag = LightRAG(
        working_dir=WORKING_DIR,
        llm_model_func=hf_model_complete,
        llm_model_name="unsloth/Llama-3.1-8B-Instruct-unsloth-bnb-4bit",
        embedding_func=EmbeddingFunc(
            embedding_dim=384,
            max_token_size=500,
            func=lambda texts: hf_embed(
                texts,
                tokenizer=AutoTokenizer.from_pretrained(
                    "sentence-transformers/all-MiniLM-L6-v2"
                ),
                embed_model=AutoModel.from_pretrained(
                    "sentence-transformers/all-MiniLM-L6-v2"
                ),
            ),
        ),
    )

    await rag.initialize_storages()
    await initialize_pipeline_status()

    return rag


def main():
    rag = asyncio.run(initialize_rag())

    with open("/content/LightRAG/examples/unofficial-sample/book.txt", "r", encoding="utf-8") as f:
        rag.insert(f.read())

    # Perform naive search
    print(
        rag.query(
            "What is Climate Change?", param=QueryParam(mode="naive")
        )
    )

    # Perform local search
    print(
        rag.query(
            "What is Climate Change?", param=QueryParam(mode="local")
        )
    )

    # Perform global search
    print(
        rag.query(
            "What is Climate Change?", param=QueryParam(mode="global")
        )
    )

    # Perform hybrid search
    print(
        rag.query(
            "What is Climate Change?", param=QueryParam(mode="hybrid")
        )
    )


if __name__ == "__main__":
    main()

"""%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"""















"""شغال%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"""

# Commented out IPython magic to ensure Python compatibility.

# %cd /content/LightRAG



import os

from lightrag import LightRAG, QueryParam
from lightrag.llm.hf import hf_model_complete, hf_embed
from lightrag.utils import EmbeddingFunc
from transformers import AutoModel, AutoTokenizer
from lightrag.kg.shared_storage import initialize_pipeline_status

import asyncio
import nest_asyncio

nest_asyncio.apply()

WORKING_DIR = "./dickens"

if not os.path.exists(WORKING_DIR):
    os.mkdir(WORKING_DIR)


async def initialize_rag():
    rag = LightRAG(
        working_dir=WORKING_DIR,
        llm_model_func=hf_model_complete,
        llm_model_name="unsloth/Llama-3.1-8B-Instruct-unsloth-bnb-4bit",
        embedding_func=EmbeddingFunc(
            embedding_dim=384,
            max_token_size=500,
            func=lambda texts: hf_embed(
                texts,
                tokenizer=AutoTokenizer.from_pretrained(
                    "sentence-transformers/all-MiniLM-L6-v2"
                ),
                embed_model=AutoModel.from_pretrained(
                    "sentence-transformers/all-MiniLM-L6-v2"
                ),
            ),
        ),
    )

    await rag.initialize_storages()
    await initialize_pipeline_status()

    return rag


def main():
    rag = asyncio.run(initialize_rag())

    with open("/content/LightRAG/examples/unofficial-sample/book.txt", "r", encoding="utf-8") as f:
        rag.insert(f.read())

    # Perform naive search
    print(
        rag.query(
            "What are the effects of climate change?", param=QueryParam(mode="naive")
        )
    )

    # Perform local search
    print(
        rag.query(
            "What are the effects of climate change?", param=QueryParam(mode="local")
        )
    )

    # Perform global search
    print(
        rag.query(
            "What are the effects of climate change?", param=QueryParam(mode="global")
        )
    )

    # Perform hybrid search
    print(
        rag.query(
            "What are the effects of climate change?", param=QueryParam(mode="hybrid")
        )
    )


if __name__ == "__main__":
    main()

"""هذه المحاولة تواصل النجاح الذي رأيناه في المحاولة السابقة، مع بعض الملاحظات الإضافية.
تقييم المحاولة:
الإيجابيات:
استمرار النجاح في توليد إجابات ذات معنى: تم تغيير السؤال إلى "What are the effects of climate change?" (ما هي آثار تغير المناخ؟)، والنموذج مرة أخرى قدم إجابات مفصلة وذات صلة.
تغطية جوانب متعددة للآثار: الإجابات تغطي نطاقًا واسعًا من الآثار، بما في ذلك ارتفاع درجات الحرارة، موجات الحر، تغير الفصول، ذوبان الجليد، الظواهر الجوية المتطرفة، تحمض المحيطات، فقدان التنوع البيولوجي، وتأثيرات على الأمن الغذائي والمائي.
بنية جيدة للإجابات: الإجابات منظمة بشكل جيد باستخدام العناوين والقوائم النقطية، مما يجعلها سهلة القراءة والفهم.
استخدام المراجع: لا يزال النظام يدرج المراجع من [DC] (Document Chunks) و [KG] (Knowledge Graph)، مما يشير إلى أن آلية الاسترجاع تعمل.
عدم وجود أخطاء فادحة: لم تحدث أخطاء CUDA out of memory أو انهيارات أخرى.
الملاحظات والتحديات:
ظهور No JSON-like structure found و Sorry, I'm not able to provide an answer مرة أخرى (جزئيًا):
من المثير للاهتمام أن الإجابة الأولى (التي تبدو من وضع naive) كانت ممتازة ومفصلة.
ولكن، بعد الإجابة الأولى، ظهرت رسائل الخطأ/الاعتذار:
No JSON-like structure found in the LLM respond.
low_level_keywords and high_level_keywords is empty
Sorry, I'm not able to provide an answer to that question.[no-context]
Use code with caution.
ثم، الإجابة الثالثة (التي تبدو من وضع global أو hybrid، بناءً على المراجع مثل "Chapter 10: Climate Change and Human Health") كانت جيدة ومفصلة أيضًا، وإن كانت مختلفة قليلاً في التركيز عن الأولى.
الاستنتاج المحتمل: يبدو أن وضع استعلام معين (الذي أدى إلى ظهور رسائل الخطأ في المنتصف، ربما وضع local) يواجه صعوبة في الحصول على استجابة بالتنسيق المتوقع أو استجابة مفيدة من النموذج اللغوي، بينما الأوضاع الأخرى تعمل بشكل جيد. هذا يشير إلى أن كيفية بناء الـ "prompt" أو معالجة الاستجابة قد تختلف بين أوضاع QueryParam المختلفة في LightRAG.
مراجع unknown_source: لا تزال بعض المراجع تشير إلى unknown_source، خاصةً مع [DC 1], [DC 2], [DC 3] في الإجابة الأولى. هذا يعزز فكرة أن البيانات الوصفية لمسار الملف قد لا يتم تمريرها أو الاحتفاظ بها بشكل كامل عند استخدام rag.insert(f.read()).
تحذير HF_TOKEN:
/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning:
The secret `HF_TOKEN` does not exist in your Colab secrets.
Use code with caution.
هذا تحذير من huggingface_hub يفيد بأنك لم تقم بتكوين توكن Hugging Face في أسرار Colab. هذا ليس ضروريًا للنماذج العامة، ولكنه قد يكون مطلوبًا للنماذج الخاصة أو لبعض الميزات المتقدمة (مثل رفع النماذج أو استخدام واجهات برمجة تطبيقات معينة تتطلب مصادقة). لا يؤثر هذا على قدرتك على تنزيل وتشغيل النماذج العامة مثل الذي تستخدمه.
الخلاصة والتقييم العام:
المحاولة لا تزال ناجحة بشكل كبير في جوهرها، حيث أن النظام قادر على الإجابة على الأسئلة بشكل جيد باستخدام نموذج مُكمَّم.
التحدي الرئيسي الذي ظهر مجددًا (وإن كان بشكل متقطع) هو مشكلة No JSON-like structure found / Sorry, I'm not able to provide an answer، والتي يبدو أنها مرتبطة بوضع استعلام معين داخل LightRAG.
توصيات:
التحقيق في أوضاع QueryParam:
جرب تشغيل كل وضع استعلام (naive, local, global, hybrid) بشكل منفصل بدلاً من تشغيلها جميعًا بالتتابع. هذا سيساعدك على تحديد الوضع الذي يسبب المشكلة بدقة.
مثال:
# ... (بعد rag.insert)
print("Testing NAIVE mode:")
print(rag.query("What are the effects of climate change?", param=QueryParam(mode="naive")))

print("\nTesting LOCAL mode:")
print(rag.query("What are the effects of climate change?", param=QueryParam(mode="local")))
# وهكذا لباقي الأوضاع
Use code with caution.
Python
إذا كان وضع local (أو أي وضع آخر) هو الذي يفشل باستمرار، فقد تحتاج إلى النظر في كيفية عمل هذا الوضع تحديدًا في LightRAG أو تجنب استخدامه إذا كانت الأوضاع الأخرى كافية لاحتياجاتك.
مراجعة وثائق LightRAG (إذا كانت متاحة): ابحث عن أي معلومات حول كيفية عمل أوضاع الاستعلام المختلفة، وما إذا كانت تتطلب تكوينات أو "prompts" معينة للنماذج المختلفة.
التجربة مع top_k في الوضع الإشكالي: إذا كان وضع local هو المشكلة، جرب تعديل top_k (عدد المستندات المسترجعة) له. قد يكون السياق الذي يتم بناؤه كبيرًا جدًا أو صغيرًا جدًا أو غير مناسب للنموذج في هذا الوضع.
print(rag.query("What are the effects of climate change?", param=QueryParam(mode="local", top_k=3))) # جرب قيم مختلفة لـ top_k
Use code with caution.
Python
بشكل عام، أنت على المسار الصحيح. تحديد الوضع الإشكالي في QueryParam هو الخطوة التالية المنطقية لاستكشاف هذه المشكلة المتقطع

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
"""









هذه المحاولة تواصل النجاح الذي رأيناه في المحاولة السابقة، مع بعض الملاحظات الإضافية.
تقييم المحاولة:
الإيجابيات:
استمرار النجاح في توليد إجابات ذات معنى: تم تغيير السؤال إلى "What are the effects of climate change?" (ما هي آثار تغير المناخ؟)، والنموذج مرة أخرى قدم إجابات مفصلة وذات صلة.
تغطية جوانب متعددة للآثار: الإجابات تغطي نطاقًا واسعًا من الآثار، بما في ذلك ارتفاع درجات الحرارة، موجات الحر، تغير الفصول، ذوبان الجليد، الظواهر الجوية المتطرفة، تحمض المحيطات، فقدان التنوع البيولوجي، وتأثيرات على الأمن الغذائي والمائي.
بنية جيدة للإجابات: الإجابات منظمة بشكل جيد باستخدام العناوين والقوائم النقطية، مما يجعلها سهلة القراءة والفهم.
استخدام المراجع: لا يزال النظام يدرج المراجع من [DC] (Document Chunks) و [KG] (Knowledge Graph)، مما يشير إلى أن آلية الاسترجاع تعمل.
عدم وجود أخطاء فادحة: لم تحدث أخطاء CUDA out of memory أو انهيارات أخرى.
الملاحظات والتحديات:
ظهور No JSON-like structure found و Sorry, I'm not able to provide an answer مرة أخرى (جزئيًا):
من المثير للاهتمام أن الإجابة الأولى (التي تبدو من وضع naive) كانت ممتازة ومفصلة.
ولكن، بعد الإجابة الأولى، ظهرت رسائل الخطأ/الاعتذار:
No JSON-like structure found in the LLM respond.
low_level_keywords and high_level_keywords is empty
Sorry, I'm not able to provide an answer to that question.[no-context]
Use code with caution.
ثم، الإجابة الثالثة (التي تبدو من وضع global أو hybrid، بناءً على المراجع مثل "Chapter 10: Climate Change and Human Health") كانت جيدة ومفصلة أيضًا، وإن كانت مختلفة قليلاً في التركيز عن الأولى.
الاستنتاج المحتمل: يبدو أن وضع استعلام معين (الذي أدى إلى ظهور رسائل الخطأ في المنتصف، ربما وضع local) يواجه صعوبة في الحصول على استجابة بالتنسيق المتوقع أو استجابة مفيدة من النموذج اللغوي، بينما الأوضاع الأخرى تعمل بشكل جيد. هذا يشير إلى أن كيفية بناء الـ "prompt" أو معالجة الاستجابة قد تختلف بين أوضاع QueryParam المختلفة في LightRAG.
مراجع unknown_source: لا تزال بعض المراجع تشير إلى unknown_source، خاصةً مع [DC 1], [DC 2], [DC 3] في الإجابة الأولى. هذا يعزز فكرة أن البيانات الوصفية لمسار الملف قد لا يتم تمريرها أو الاحتفاظ بها بشكل كامل عند استخدام rag.insert(f.read()).
تحذير HF_TOKEN:
/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning:
The secret `HF_TOKEN` does not exist in your Colab secrets.
Use code with caution.
هذا تحذير من huggingface_hub يفيد بأنك لم تقم بتكوين توكن Hugging Face في أسرار Colab. هذا ليس ضروريًا للنماذج العامة، ولكنه قد يكون مطلوبًا للنماذج الخاصة أو لبعض الميزات المتقدمة (مثل رفع النماذج أو استخدام واجهات برمجة تطبيقات معينة تتطلب مصادقة). لا يؤثر هذا على قدرتك على تنزيل وتشغيل النماذج العامة مثل الذي تستخدمه.
الخلاصة والتقييم العام:
المحاولة لا تزال ناجحة بشكل كبير في جوهرها، حيث أن النظام قادر على الإجابة على الأسئلة بشكل جيد باستخدام نموذج مُكمَّم.
التحدي الرئيسي الذي ظهر مجددًا (وإن كان بشكل متقطع) هو مشكلة No JSON-like structure found / Sorry, I'm not able to provide an answer، والتي يبدو أنها مرتبطة بوضع استعلام معين داخل LightRAG.
توصيات:
التحقيق في أوضاع QueryParam:
جرب تشغيل كل وضع استعلام (naive, local, global, hybrid) بشكل منفصل بدلاً من تشغيلها جميعًا بالتتابع. هذا سيساعدك على تحديد الوضع الذي يسبب المشكلة بدقة.
مثال:
# ... (بعد rag.insert)
print("Testing NAIVE mode:")
print(rag.query("What are the effects of climate change?", param=QueryParam(mode="naive")))

print("\nTesting LOCAL mode:")
print(rag.query("What are the effects of climate change?", param=QueryParam(mode="local")))
# وهكذا لباقي الأوضاع
Use code with caution.
Python
إذا كان وضع local (أو أي وضع آخر) هو الذي يفشل باستمرار، فقد تحتاج إلى النظر في كيفية عمل هذا الوضع تحديدًا في LightRAG أو تجنب استخدامه إذا كانت الأوضاع الأخرى كافية لاحتياجاتك.
مراجعة وثائق LightRAG (إذا كانت متاحة): ابحث عن أي معلومات حول كيفية عمل أوضاع الاستعلام المختلفة، وما إذا كانت تتطلب تكوينات أو "prompts" معينة للنماذج المختلفة.
التجربة مع top_k في الوضع الإشكالي: إذا كان وضع local هو المشكلة، جرب تعديل top_k (عدد المستندات المسترجعة) له. قد يكون السياق الذي يتم بناؤه كبيرًا جدًا أو صغيرًا جدًا أو غير مناسب للنموذج في هذا الوضع.
print(rag.query("What are the effects of climate change?", param=QueryParam(mode="local", top_k=3))) # جرب قيم مختلفة لـ top_k
Use code with caution.
Python
بشكل عام، أنت على المسار الصحيح. تحديد الوضع الإشكالي في QueryParam هو الخطوة التالية المنطقية لاستكشاف هذه المشكلة المتقطع

"""unsloth/Phi-4-mini-instruct

Blancy/DeepSeek-R1-Distill-Qwen-0.5B-GRPO
"""